<html><head><title>HIPI: Downloader</title></head><body bgcolor="#ffffff">

<body>
<h1 align="center">HIPI Downloader Tool</h1>
<div align="center"><img src="flicker_to_hadoop.png" /></div>

<h2>Goals</h2>
<ul><li>Extract images from internet
<li>Organize them in a logical way
<li>Store them efficiently
<li>Simple access after storage
<li>SIMPLICITY!
</ul>

<h2>Steps</h2>
<ol><li>Specify a list of images to collect
<li>Split urls into groups, send each group to a Mapper
<li>Download images from internet
<li>Store images in a HipiImageBundle
</ol>
<div align="center"><img src="downloader.png" width="500"/></div>
The image above depicts the natural pipeline for the downloader
<h3>Step 1: Specifiy a list of images to collect</h3>
<p>We assume that there exists a well-formed list containing url's of images to download. This list should be stored in a text file with exactly one image url per line. This list can be generated by hand, by MySQL, or from a search query (e.g. google images, flickr, etc.)</p>

<p>
In addition to the list of images, the user will input the number of nodes to run the task. According to this input, we will divide the image set across the specified number of nodes for maximum efficiency and parallelism when downloading the images. Each node in the Map task will generate a HipiImageBundle containing all of them images it downloaded, then the Reducer will merge all the HipiImageBundles together to form one large HipiImageBundle.
</p>

<h3>Step 2: Split URLs into groups and send each group to a Mapper</h3>
<p>Using the inputted list of image urls and the number of nodes used to download these images, we will equally distribute the task of downloading images to the specified number of map nodes. This allows for maximum parallelization for the downloading process. Our DownloaderInputFormat class allocates the proper number of nodes based on the user-inputted value.
</p>
<p>Once the image sets have been distributed to the various nodes, the map tasks will begin running. In other words, each map task will have a set of images to download. Each map task will iterate over these sets to download every image. The mappers retrieve the url of the image to be downloaded for all images in every set specified.
</p>

<h3>Step 3: Download images from the internet</h3>
We then establish a connection to the url retrieved from the database and download the image using java's URLConnection class. Once connected, we check the file type to make sure it is an image (currently we only accept JPEG images), and get an InputStream to the connection. From this point we can use the InputStream to add the image to a HipiImageBundle (detailed below). Each map task will create a HIPI Image Bundle that contains the images it has downloaded, ensuring everything will be done in parallel.


<h3>Step 4: Store images in a HipiImageBundle</h3>
<p>We store images in HIPI Image Bundles in the Map phase, then merge the various HIPI Image Bundles together in the reduce phase. The results of the reduce phase is one large HIPI Image Bundle containing all of the images that have been downloaded. This way, we ensure that we achieve maximum parallelization while downloading, and also can achieve the benefits of storing images in a HIPI Image Bundle with the final result.</p>
<p> HipiImageBundle provides an intuitive interface for dealing with image sets. HipiImageBundles create an index file containing the image offsets, and a data file that contains image files concatenated with each other. The diagram below demonstrates the file structure for HipiImageBundles
</p>
<div align="center"><img src="hib.png" /></div>

<p>By storing images this way, you are able to take advantage of our HIPI framework for MapReduce tasks. For example, to check the results of the Downloader program, I ran a very simple MapReduce Program (7 lines) that was able to take the HipiImageBundle and write out the images to individual JPEG files effortlessly.
</p>

</body>
</html>

