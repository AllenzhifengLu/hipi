<!DOCTYPE html>
<html>
  <head>
    <meta name="Description" content="HIPI - Hadoop Image Processing Interface getting started page tells you what you need to know to start using HIPI on Hadoop MapReduce." />
    <meta charset="UTF-8">
    
    <link rel="stylesheet" type="text/css" href="include/main.css" />
    <link rel="stylesheet" type="text/css" href="include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Getting Started</title>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-23539446-1']);
      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
      </script>

    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
    <style> pre.prettyprint {padding:20px 20px 0px 20px;} </style>

  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="index.html">Overview</a></li>
	<li><a href="gettingstarted.html">Getting Started</a></li>
	<li><a href="documentation.html">Documentation</a></li>
	<li><a href="examples.html">Tools and Examples</a></li>
	<li><a href="downloads.html">Downloads</a></li>
	<li><a href="about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">

      <div class="section">
      
	<h3>1. Setup Hadoop</h3>

        HIPI works with a standard installation of the Apache Hadoop Distributed File System (HDFS) and MapReduce. HIPI has been tested with Hadoop version 2.6.0.<br /><br />
	
        If you haven't already done so, download and install Hadoop by following the instructions on the official <a class="external_link" href="http://hadoop.apache.org/">Apache Hadoop website</a>. A very useful resource is their <a class="external_link" href="http://wiki.apache.org/hadoop/QuickStart">Quickstart Guide</a>, in particular, the <a class="external_link" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">Single Cluster Setup</a> for first-time users.<br /><br />

        Ensure that the Hadoop bin directory is in your system path. You can verify that this is the case with the Unix program <tt>which</tt>:

	<pre class="console">
$> which hadoop
/usr/local/bin/hadoop
	</pre>

	The <a href="examples.html">example programs</a> section of this website assumes that the main hadoop script is in the system path.

        <h3>2. Install Apache Ant</h3>

	The HIPI distribution uses <a class="external_link" href="http://ant.apache.org/">Apache Ant</a> for code compilation. HIPI has been tested with Apache Ant version 1.9.4.<br /><br />

	Install ant on your system and verify that it is in your path as well:

	<pre class="console">
$> which ant
/usr/local/bin/ant
	</pre>	

	<h3>3. Install HIPI</h3>

	There are two ways to install HIPI on your system:
	<ol>
	  <li> Clone the latest HIPI distribution from GitHub and build from source. <span class="important">(Recommended)</span> </li>
	  <li> Download a precompiled JAR from the <a href="downloads.html">downloads</a> page.</li>
	</ol>

	<h3>Clone the HIPI GitHub Repository</h3>

	The best way to get the latest version of HIPI is by cloning the <a class="external_link" href="https://github.com/uvagfx/hipi">official GitHub repository</a> and building it along with all of the tools and example programs yourself. This only takes a second and verifies that your system is properly setup and you are ready to begin developing your own HIPI applications:

	<pre class="console">
git clone git@github.com:uvagfx/hipi.git
	</pre>	

	<span class="important">Important</span> After the repository has been cloned, you must edit two lines in the <tt>build.xml</tt> file in the HIPI root directory to indicate the path to your Hadoop installation and the version of Hadoop you are using:

	<pre class="prettyprint">
build.xml:
  ...
  &#60;property name="hadoop.home" value="/opt/hadoop-2.6.0/share/hadoop" /&#62;
  &#60;property name="hadoop.version" value="2.6.0" /&#62;
  ...
	</pre>

	The correct value of <tt>hadoop.home</tt> and <tt>hadoop.version</tt> may differ on your system from what is shown above. For example, Hadoop version 2.5.1 on a Mac OS X system installed using homebrew might have the following:

	<pre class="prettyprint">
build.xml:
  ...
  &#60;property name="hadoop.home" value="/usr/local/Cellar/hadoop/2.5.1/libexec/share/hadoop" /&#62;
  &#60;property name="hadoop.version" value="2.5.1" /&#62;
  ...
	</pre>

	<h3>Build the HIPI Library and Example Programs</h3>

	From the HIPI root directory, simply run <tt>ant</tt> to build the HIPI library along with all of the tools and example programs:

	<pre class="console">
$> ant
...
hipi:
    [javac] Compiling 30 source files to /Users/jason/work/proj/hipi/lib
      [jar] Building jar: /Users/jason/work/proj/hipi/lib/hipi-2.0.jar
     [echo] Hipi library built.

compile:
    [javac] Compiling 1 source file to /Users/jason/work/proj/hipi/bin
      [jar] Building jar: /Users/jason/work/proj/hipi/examples/covariance.jar
     [echo] Covariance built.

all:

BUILD SUCCESSFUL
Total time: 3 seconds
	</pre>

	If the build fails, first carefully review the steps above. If you are convinced that you are doing everything correctly and that you've found an issue with the HIPI distribution or documentation please post a question to the <a class="external_link" href="https://groups.google.com/forum/#!forum/hipi-users">HIPI Users Group</a> or <a class="external_link" href="https://github.com/uvagfx/hipi/issues">file a bug report</a>.<br /><br />

	After the build successfully finishes, open the <tt>build.xml</tt> file in a text editor and have a look at the various build tasks. They should be straightforward. For example, to build only the <a href="examples/dumphib.html">DumpHib example program</a> you would execute:

	<pre class="console">
$> ant dumphib
Buildfile: /users/horton/hipi/build.xml

dumphib:
     [echo] Building dumphib example...
...
compile:
    [javac] Compiling 1 source file to /Users/jason/work/proj/hipi/bin
      [jar] Building jar: /Users/jason/work/proj/hipi/examples/dumphib.jar
     [echo] Dumphib built.

BUILD SUCCESSFUL
Total time: 1 second
	</pre>

	HIPI is now installed on your system. To learn about future updates to the HIPI distribution you should join the <a class="external_link" href="https://groups.google.com/forum/#!forum/hipi-users">HIPI Users Group</a> and <a class="external_link" href="https://github.com/uvagfx/hipi">watch the HIPI GitHub repository</a>. You can always obtain the latest version of HIPI on the release branch with the following <tt>git pull</tt> command:

	<pre class="console">
$> git pull origin release
From github.com:uvagfx/hipi
 * branch            release    -> FETCH_HEAD
Already up-to-date.
	</pre>

	Also, you can experiment with the <tt>development</tt> branch, which contains the latest features that have not yet been integrated into the main release branch. Note that the <tt>development</tt> branch is generally less stable than the <tt>release</tt> branch.<br /><br />

	Next, we walk you through the process of writing your first HIPI program. Be sure to also check out the <a href="examples.html">tools and examples page</a> to learn much more about HIPI.

      </div>

      <h3>Your First HIPI Program</h3>

      <div class="section">

      This section will walk you through the process of creating a very simple HIPI program that computes the average pixel color over a set of images. First, we need a set of images to work with. The primary input type to a HIPI program is a <a href="doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a> (HIB), which stores a collection of images on the Hadoop Distributed File System (HDFS). Use the <b>hibimport</b> tool to create a HIB from a collection of images on your local file system located in the directory <tt>~/SampleImages</tt> by executing the following command from the HIPI root directory:

	<pre class="console">
$> hadoop jar tool/hibimport.jar ~/SampleImages sampleimages.hib
** added: 1.jpg
** added: 2.jpg
** added: 3.jpg
Created: sampleimages.hib and sampleimages.hib.dat
        </pre>

	<b>dumphib</b> creates two files in the current working directory of the HDFS: <tt>sampleimages.hib</tt> and <tt>sampleimages.hib.dat</tt>. You can verify that this is the case with the command: <tt>hadoop fs -ls</tt> (Learn how the <b>hibimport</b> tool works <a href="tools/hibimport.html">here</a> after finishing this tutorial.)<br /><br />

	Next, we will add an Ant build rule for our new program by modifying the <tt>build.xml</tt> file in the HIPI root directory. Open <tt>build.xml</tt> file in a text editor and add a new target:

	<pre class="prettyprint">
...
&#60;target name="firstprog"&#62;
  &#60;antcall target="compile"&#62;
    &#60;param name="srcdir" value="firstprog" /&#62;
    &#60;param name="jarfilename" value="firstprog.jar" /&#62;
    &#60;param name="jardir" value="firstprog" /&#62;
    &#60;param name="mainclass" value="FirstProgram" /&#62;
  &#60;/antcall&#62;
&#60;/target&#62;
...
	</pre>
		  
	Next, create the directory <tt>firstprog</tt> in the root HIPI directory along with a new Java file named <tt>firstprog/FirstProgram.java</tt> that contains the following code:

	<pre class="prettyprint">
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class FirstProgram extends Configured implements Tool {

  public int run(String[] args) throws Exception {
    System.out.println("Hello HIPI!");
    return 0;
  }

  public static void main(String[] args) throws Exception {
    ToolRunner.run(new FirstProgram(), args);
    System.exit(0);
  }

}
	</pre>

	The entry point of every Java program is the <tt>public static void main()</tt> method. As in most MapReduce applications, it uses the <tt>ToolRunner</tt> Hadoop object to call the <tt>run()</tt> method in this driver class.<br /><br />
	
	Build this very simple program by running <tt>ant firstprog</tt>. If the compilation succeeds, the file <tt>firstprog.jar</tt> will be located in the <tt>firstprog</tt> directory. Run this program using the following command:

	<pre id="Current">
  $> hadoop jar firstprog/firstprog.jar
  Hello HIPI!
        </pre>

	Congratulations! You just created a very simple MapReduce program. Now let's make our program do some image processing with HIPI.<br /><br />

	<h3>MapReduce</h3>

	Hadoop's MapReduce parallel programming framework is a very powerful (and very popular) tool for large-scale distributed computing. If this is your first experience with MapReduce, we recommend reading the official <a class="external_link" href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Apache MapReduce tutorial</a>, which gives a nice introduction to this programming model. Another great read is the paper written by Jeffrey Dean and Sanjay Ghemawat at Google titled <a class="external_link" href="http://research.google.com/archive/mapreduce.html">MapReduce: Simplified Data Processing on Large Clusters</a>.<br /><br />

	Next, let's extend the <tt>run()</tt> method in <tt>FirstProgram.java</tt> to initialize and execute a MapReduce job and create stubs for our Mapper and Reducer classes:

	<pre class="prettyprint">
import hipi.image.FloatImage;
import hipi.image.ImageHeader;
import hipi.imagebundle.mapreduce.ImageBundleInputFormat;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class FirstProgram extends Configured implements Tool {
  
  public static class FirstProgramMapper extends Mapper&#60;ImageHeader, FloatImage, IntWritable, FloatImage&#62; {
    public void map(ImageHeader key, FloatImage value, Context context) 
      throws IOException, InterruptedException {
    }
  }
  
  public static class FirstProgramReducer extends Reducer&#60;IntWritable, FloatImage, IntWritable, Text&#62; {
    public void reduce(IntWritable key, Iterable&#60;FloatImage&#62; values, Context context) 
      throws IOException, InterruptedException {
    }
  }
  
  public int run(String[] args) throws Exception {
    // Check input arguments
    if (args.length != 2) {
      System.out.println("Usage: firstprog &#60;input HIB&#62; &#60;output directory&#62;");
      System.exit(0);
    }
    
    // Initialize and configure MapReduce job
    Job job = Job.getInstance();
    // Set input format class which parses the input HIB and spawns map tasks
    job.setInputFormatClass(ImageBundleInputFormat.class);
    // Set the driver, mapper, and reducer classes which express the computation
    job.setJarByClass(FirstProgram.class);
    job.setMapperClass(FirstProgramMapper.class);
    job.setReducerClass(FirstProgramReducer.class);
    // Set the types for the key/value pairs passed to/from map and reduce layers
    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(FloatImage.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputValueClass(Text.class);
    
    // Set the input and output paths on the HDFS
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    // Execute the MapReduce job and block until it complets
    boolean success = job.waitForCompletion(true);
    
    // Return success or failure
    return success ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    ToolRunner.run(new FirstProgram(), args);
    System.exit(0);
  }
  
}
	</pre>
	
	Most of this code imports necessary Hadoop and HIPI libraries and configures and launches the MapReduce <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/jobcontrol/Job.html">Job</a>. This type of code will become somewhat boilerplate across the MapReduce/HIPI programs you develop, but it's still important to understand what is going on. The first lines of code in the <tt>run()</tt> method validate the arguments passed to the program, create the Hadoop Job object and call setter methods on this object to specify the classes that implement the map and reduce tasks along with the types of objects that are passed to and from these processing stages. The remaining lines of code setup the path to the input file and the output directory and launch the program. The descriptions in the <a href="examples.html">example programs page</a> provide much more detail about these parts of a HIPI program, which we will skip for now. Instead we will focus on the more interesting bits of the program that will appear in the <tt>map()</tt> and <tt>reduce()</tt> methods.<br /><br />

	Before proceeding further, test that your code still compiles and runs, but don't expect it to do anything yet.

	<h3>Computing The Average Pixel Color</h3>

	Now let's add some actual HIPI image processing code to our program. For this example, we will be computing the average RGB value of the pixels in the images in our input HIB. Our mapper will compute the average pixel color over a single image and the reducer will add these averages together and divide by their count to compute the total average pixel color. Because the map tasks are executed in parallel, if our Hadoop cluster has more than one compute node we will perform this entire operation faster than if we were using a single machine. This is the key idea behind parallel computing with MapReduce.<br /><br />

	Here is what our <tt>map()</tt> method looks like:

	<pre class="prettyprint">
  public static class FirstProgramMapper extends Mapper&#60;ImageHeader, FloatImage, IntWritable, FloatImage&#62; {
    
    public void map(ImageHeader key, FloatImage value, Context context) 
        throws IOException, InterruptedException {

      // Verify that image was properly decoded, is of sufficient size, and has three color channels (RGB)
      if (value != null && value.getWidth() &#62; 1 && value.getHeight() &#62; 1 && value.getBands() == 3) {

        // Get dimensions of image
        int w = value.getWidth();
        int h = value.getHeight();

        // Get pointer to image data
        float[] valData = value.getData();

        // Initialize 3 element array to hold RGB pixel average
        float[] avgData = {0,0,0};

        // Traverse image pixel data in raster-scan order and update running average
        for (int j = 0; j < h; j++) {
          for (int i = 0; i < w; i++) {
            avgData[0] += valData[(j*w+i)*3+0]; // R
            avgData[1] += valData[(j*w+i)*3+1]; // G
            avgData[2] += valData[(j*w+i)*3+2]; // B
          }
        }

        // Create a FloatImage to store the average value
        FloatImage avg = new FloatImage(1, 1, 3, avgData);

        // Divide by number of pixels in image
        avg.scale(1.0f/(float)(w*h));

        // Emit record to reducer
        context.write(new IntWritable(1), avg);

      } // If (value != null...
      
    } // map()

  } // FirstProgramMapper
	</pre>

	This <tt>map()</tt> method creates one key/value pair (called a "record" in MapReduce terminology) for each image in the HIB consisting of an IntWritable always equal to 1 and a HIPI FloatImage object that contains the image's computed average pixel value, respectively. These records are combined by the MapReduce framework and presented to the <tt>reduce()</tt> method as an Iterable list of FloatImage objects where they are added together and normalized to obtain the final result:

	<pre class="prettyprint">
  public static class FirstProgramReducer extends Reducer&#60;IntWritable, FloatImage, IntWritable, Text&#62; {

    public void reduce(IntWritable key, Iterable&#60;FloatImage&#62; values, Context context)
        throws IOException, InterruptedException {

      // Create FloatImage object to hold final result
      FloatImage avg = new FloatImage(1, 1, 3);

      // Initialize a counter and iterate over IntWritable/FloatImage records from mapper
      int total = 0;
      for (FloatImage val : values) {
        avg.add(val);
        total++;
      }

      if (total > 0) {
        // Normalize sum to obtain average
        avg.scale(1.0f / total);
        // Assemble final output as string
	float[] avgData = avg.getData();
        String result = String.format("Average pixel value: %f %f %f", avgData[0], avgData[1], avgData[2]);
        // Emit output of job which will be written to HDFS
        context.write(key, new Text(result));
      }

    } // reduce()

  } // FirstProgramReducer
	</pre>

	Compile your program and run it on the HIB we created at the beginning:

	<pre class="console">
$> ant firstprog
...
BUILD SUCCESSFUL
Total time: 1 second
$> hadoop jar firstprog/firstprog.jar sampleimages.hib sampleimages_average
...
        </pre>

	If everything goes as planned the directory <tt>sampleimages_average</tt> will contain two files:

	<pre class="console">
$> hadoop fs -ls sampleimages_average
Found 2 items
-rw-r--r--   1 user group        0 2015-03-13 09:52 sampleimages_average/_SUCCESS
-rw-r--r--   1 user group       50 2015-03-13 09:52 sampleimages_average/part-r-00000
        </pre>

	Whenever a MapReduce program successfully finishes, it creates the file <tt>_SUCCESS</tt> in the output directory along with a <tt>part-r-XXXXX</tt> file for each reduce task. The average pixel value can be retrieved using the <tt>cat</tt> command:

	<pre class="console">
$> hadoop fs -cat sampleimages_average/part-r-00000
1	Average pixel value: 0.555262 0.487656 0.366019
        </pre>

	Play around with different image sets and see how it affects the average pixel color. (Note: you will need to remove the output directory before running the program a second time with the command: <tt>hadoop fs -rm -R sampleimages_average</tt>.

	<h3>Next</h3>

        Read the descriptions of the other <a href="examples.html">tools and example programs</a> or jump into the <a href="documentation.html">documentation</a> to learn more about HIPI.

    </div>
    <!-- End Content -->
  </body>
</html>
  
