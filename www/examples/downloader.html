<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="Description" content="Hadoop Image Processing Interface example for creating a HIB from a set of ,potentially Flickr, image URLs in a distributed manner." />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Distributed Downloader Example</title>
  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="../index.html">Introduction</a></li>
	<li><a href="../gettingstarted.html">Getting Started</a></li>
	<li><a href="../doc/api/index.html">Documentation</a></li>
	<li><a href="../examples.html">Examples</a></li>
	<li><a href="../downloads.html">Downloads</a></li>
	<li><a href="../about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">
      
      <h2>The HIPI Distributed Downloader</h2>
      <div class="section">
	
	The HIPI Distributed Downloader is an example program that
	creates a <a href="">HipiImageBundle</a> from a list of image
	URL&#39;s. This list can be generated by first
	&#39;scraping&#39; a website that contains organized photo
	URL's, such as <a class="external_link"
	href="http://www.flickr.com">Flickr</a> or <a
	class="external_link" href="http://images.google.com">Google
	Images</a>.
	
	<h3>Compiling the Example</h3>
	
	Before you can compile this (or any other example) you must
	configure the compiling script so that it knows where your
	Hadoop installation resides. Open up the build.xml file in the
	root directory of your HIPI installation. At the top of the
	file are two important properties that are left blank named
	<strong>hadoop.home</strong> and
	<strong>hadoop.version</strong>. Fill in the value attributes
	of these two properties with the location of your Hadoop
	installation and your Hadoop version. For instance, if you
	downloaded Hadoop 0.20.1 and unpacked it to
	/hadoop/hadoop-0.20.1, then you would have the following
	build.xml file:
	<pre id="Current">
  &#60;project basedir="." default="all"&#62;
	  
  &#60;target name="setup"&#62;
  &#60;property name="hadoop.home" value="/hadoop/hadoop-0.20.1" /&#62;
  &#60;property name="hadoop.version" value="0.20.1" /&#62;
  &#60;property name="hadoop.classpath" value="${hadoop.home}/hadoop-${hadoop.version}-core.jar" /&#62;
  &#60;property name="metadata.jar" value="3rdparty/metadata-extractor-2.3.1.jar" /&#62;
  &#60;/target&#62;
  ...
          </pre><br />
	  
	  You can compile this example by executing the following
	  command in the root directory where you unpacked HIPI:
	  <pre id="Current">
  $> ant downloader
	    </pre><br />
	    
	    This will run an ANT script that will create the file
	    examples/download.jar. If you navigate to the examples folder
	    and execute the following command, you should see the usage
	    message for the downloader program:
	    <pre id="Current">
  $> ./runDownloader.sh
	    </pre>
	    
	    <div class="important">Important Note:</div> You must be
	    using Java JDK version 1.6 in order to ensure that HIPI
	    will compile correctly. Although ealrier versions
	    <em>may</em> work, we have not fully tested them.
	    
	    <h3>Running the Example</h3> 
	    
	    The downloader program takes three command line parameters,
	    which you can specify directly to the
	    <tt>runDownloader.sh</tt> script. The first parameter is the
	    Hadoop Distributed File System (HDFS) path to the list of
	    URL's to be downloaded. Each URL must reside as an ASCII
	    string on its own line in this file. The second parameter is
	    the HDFS path to the output file that will be created once the
	    program has finished. The third and final parameter specifies
	    the number of nodes that will be used to distribute the
	    downloader. This should be set in accordance with the
	    bandwidth your system can sustain. For example:
	    <pre id="Current">
  $> ./runDownloader.sh /hdfs/path/to/list.txt /hdfs/path/to/output.hib 100
	    </pre><br />
	      
	    In this case, 100 nodes will download the images specified in
	    the list.txt file and will be stored as a <a
	    href="">HipiImageBundle</a> in the output.hib file. If the
	    list of URL's contains 100,000 images, then each of the 100
	    nodes in the cluster will be responsible for downloading 1,000
	    images. In general this has proven to be a good number of
	    images per node to use.
      </div>
      
      <h2>Understanding the Downloader</h2>
      <div class="section">
	
	The <a href="">DistributedDownloader</a> attempts to download
	a list of URL's specified in an input ASCII text file with one
	URL per line. Since it is often the case that downloading a
	large set of images from a single computer can be time
	prohibitive and because typical cluster setups can support
	more bandwidth than one machine is able to saturate, our
	downloader attempts to distribute this task to a subset of the
	nodes in a Hadoop setup.<br /><br />

	The Distributed Downloader's <a
	href="">DownloaderInputFormat</a> class is a specialized
	version of the standard Hadoop <a
	class="external_link">FileInputFormat</a> class that is
	capable of forcing a user-specified number of nodes
	(<strong>n</strong>) in a Hadoop cluster to process a set of
	test lines in a file. This input format causes
	<strong>n</strong> nodes to download (total # of images) /
	<strong>n</strong> images into a local <a
	href="">HipiImageBundle</a>. Once all of these tasks have
	completed, a Reduce phase merges all of the HIB's into one
	final output HIB. The following image illustrates this
	process:

	<img class="centered_image" src="downloader.png" width="500" alt="" />
	
	<h3>The Input File</h3>
	
	The input file is just a list of URL's represented as ASCII
	strings and occupying exactly one line a piece:
	<pre id="Current">
  http://farm3.static.flickr.com/2413/1637516180_7d7aac7831_b.jpg
  http://farm3.static.flickr.com/2386/2016921042_c351deb735_b.jpg
  http://farm3.static.flickr.com/2395/2016921570_5935e4cb07_b.jpg
  http://farm2.static.flickr.com/1054/692713810_8bc84408c1_b.jpg
  ...
	</pre>
	In this case, the image URL's were gathered by querying the <a
	class="external_link" href="">Flickr API</a> in a separate
	program.

	<h3>DownloaderInputFormat</h3> Under normal operation, Hadoop
	attempts to move the executable code for a job to the nodes in
	the cluster that contain the data to be operated on. In the
	case of the Downloader, this is a problem because the
	&#39;data&#39; in this case is just a set of URL's, which in
	most cases will all exist on one node. Thus in order to coerce
	Hadoop into behaving appropriately in this context, special
	care must be taken.<br /><br />

	<div class="important">Important Note: </div> This portion of
	the explanation can be skipped as it is not related to HIPI as
	much as it is to Hadoop.<br /><br />

	The <a href="">DownloaderInputFormat</a> class tricks Hadoop
	into spawning a user-specified number of nodes by creating
	temporary files on the Hadoop Distributed File System
	(HDFS). Since Hadoop will attempt to load-balance files, each
	new file has a high probability of existing on a different
	machine. The process to allocate <strong>n</strong> nodes is:
	<ol>
	  <li>
	    Create a temporary file in a random location on the HDFS
	    <pre id="Current">
  ...
  ArrayList&lt;String&gt; hosts = <font id="New">new</font> ArrayList&lt;String&gt;(<font id="IntegerLiteral">0</font>);
  FileSystem fileSystem = FileSystem.get(conf);
  String tempOutputPath = conf.get(<font id="StringLiteral">"downloader.outpath"</font>) + <font id="StringLiteral">"_tmp"</font>;
  Path tempOutputDir = <font id="New">new</font> Path(tempOutputPath);
  <font id="Int">int</font> i = <font id="IntegerLiteral">0</font>;
  <font id="While">while</font>( hosts.size() &lt; nodes &amp;&amp; i &lt; <font id="IntegerLiteral">2</font>*nodes)
  {
    String tempFileString = tempOutputPath + <font id="StringLiteral">"/"</font> + i;
    Path tempFile = <font id="New">new</font> Path(tempFileString);
    FSDataOutputStream os = fileSystem.create(tempFile);
    os.write(i);
    os.close();
	    </pre>
	  </li>
	  <li>
	    Determine which node(s) the temporary file was actually saved to
	    <pre id="Current">
  FileStatus match = fileSystem.getFileStatus(tempFile);
  <font id="Long">long</font> length = match.getLen();
  BlockLocation[] blocks = fileSystem.getFileBlockLocations(match, <font id="IntegerLiteral">0</font>, length);
	    </pre>
	  </li>
	  <li>
	    Ensure this node is unique, otherwise start over
	    <pre id="Current">
  <font id="Boolean">boolean</font> save = <font id="True">true</font>;
  <font id="For">for</font> (<font id="Int">int</font> j = <font id="IntegerLiteral">0</font>; j &lt; hosts.size(); j++)
  {
      <font id="If">if</font> (blocks[<font id="IntegerLiteral">0</font>].getHosts()[<font id="IntegerLiteral">0</font>].compareTo(hosts.get(j)) == <font id="IntegerLiteral">0</font>)
      {
          save = <font id="False">false</font>;
          <font id="Break">break</font>;
      }
  }
	    </pre>
	  </li>
	  <li>
	    If the node was unique, save it in a list of nodes and repeat until the list contains <strong>n</strong> nodes
	    <pre id="Current">
  <font id="If">if</font> (save)
  {
    hosts.add(blocks[<font id="IntegerLiteral">0</font>].getHosts()[<font id="IntegerLiteral">0</font>]);
  }
  i++;
	    </pre>
	  </li>
	</ol>

	The resulting list of hosts can be used to generate a <a
	class="external_link" href="">FileSplit</a> for each node. The
	FileSplit in this case is utilized in an unorthodox way to
	indicate to each Map node which portion of the URL's it will
	be responsible for. The constructor for the FileSplit normally
	takes four arguments: the path to the file, the starting byte
	offset for the split, the byte length of the split, and the
	list of hosts where the split resides. Instead of using this
	convention, the start offset and length parameters are
	substituted with the line offset and number of lines to be
	processed:
	<pre id="Current">
  <font id="Int">int</font> span = (<font id="Int">int</font>) Math.ceil(((<font id="Float">float</font>) (num_lines)) / ((<font id="Float">float</font>) hosts.size()));
  <font id="Int">int</font> last = num_lines - span * (hosts.size()-<font id="IntegerLiteral">1</font>);
  
  FileSplit[] f = <font id="New">new</font> FileSplit[hosts.size()];
  <font id="For">for</font> (<font id="Int">int</font> j = <font id="IntegerLiteral">0</font>; j &lt; f.length; j++) 
  {
      String[] host = <font id="New">new</font> String[<font id="IntegerLiteral">1</font>];
      host[<font id="IntegerLiteral">0</font>] = hosts.get(j);
      <font id="If">if</font> (j &lt; f.length - <font id="IntegerLiteral">1</font>) 
      {
          splits.add( <font id="New">new</font> FileSplit(path , (j*span) , span, host));
      } <font id="Else">else</font> 
      {
          splits.add( <font id="New">new</font> FileSplit(path , (j*span) , last, host));
      }
  }
	</pre>

	<h3>DownloaderRecordReader</h3>

	The <a class="external_link" href="">RecordReader</a> is
	normally tasked with emitting a set of records for each <a
	class="external_link" href="">InputSplit</a> it receives from
	a specified <a class="external_link"
	href="">InputFormat</a>. In the case of the Downloader, the
	DownloaderRecordReader is responsible for communicating the to
	the Map task which lines in the input list of URL's it is
	responsible for downloading. As mentioned above, the
	DownloaderInputFormat class uses the <a class="external_link"
	href="">FileSplit</a> object to hold this information.<br
	/><br />

	For each FileSplit, the DownloaderRecordReader emits exactly
	one record whose key indicates the starting line number and
	whose value contains the list of URL's to be processed. The
	DownloaderRecordReader first determines the starting line
	number for the FileSplit and the number of lines that will be
	processed:
	<pre id="Current">
  ...
  <font id="Public">public</font> <font id="Void">void</font> initialize(InputSplit split, TaskAttemptContext context) <font id="Throws">throws</font> IOException, InterruptedException {
    FileSplit f = (FileSplit) split;
    Path path = f.getPath();
  
    start_line = f.getStart();
    <font id="Long">long</font> num_lines = f.getLength();
	</pre>

	The corresponding URL's are determined by reading
	<strong>num_lines</strong> from the input file starting at
	line <strong>start_line</strong>:
	<pre id="Current">
  BufferedReader reader = <font id="New">new</font> BufferedReader(<font id="New">new</font> InputStreamReader(fs.open(path)));
  <font id="Int">int</font> i = <font id="IntegerLiteral">0</font>;
  <font id="While">while</font>(i &lt; start_line &amp;&amp; reader.readLine() != <font id="Null">null</font>){
    i++;
  }
  
  urls = <font id="StringLiteral">""</font>;
  String line;
  <font id="For">for</font>(i = <font id="IntegerLiteral">0</font>; i &lt; num_lines &amp;&amp; (line = reader.readLine()) != <font id="Null">null</font>; i++){
    urls += line + <font id="CharacerLiteral">'\n'</font>;
  }
  reader.close();
	</pre>

	Map tasks receive their records by repeatedly calling <a
	class="external_link"
	href="">RecordReader::nextKeyValue()</a>, <a
	class="external_link"
	href="">RecordReader::getCurrentKey()</a>, and <a
	class="external_link"
	href="">RecordReader::getCurrentValue()</a>. Thus, in this
	case, there is only one record whose key and value are just
	the starting line number and the list of URL's respectively:
	<pre id="Current">
  <font id="Public">public</font> IntWritable getCurrentKey() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="Return">return</font> <font id="New">new</font> IntWritable((<font id="Int">int</font>)start_line);
  }

  <font id="Public">public</font> Text getCurrentValue() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="Return">return</font> <font id="New">new</font> Text(urls);
  }

  <font id="Public">public</font> <font id="Boolean">boolean</font> nextKeyValue() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="If">if</font>(singletonEmit == <font id="False">false</font>){
          singletonEmit = <font id="True">true</font>;
          <font id="Return">return</font> <font id="True">true</font>;
      }
      <font id="Else">else</font>
          <font id="Return">return</font> <font id="False">false</font>;
  }
	</pre>

	<h3>The Mapper</h3>

	The <a class="external_link" href="">RecordReader</a> emits
	records to the <a class="external_link" href="">Mapper</a>
	task where the keys are the starting line in the input list of
	URL's and the values are the URL's to be downloaded (separated
	again by newline characters).<br /><br />

	The Mapper's task is to download each of the URL's to a local
	<a href="">HipiImageBundle</a> (HIB), which will then be sent
	to the <a class="external_link" href="">Reducer</a> so it can
	be combined with the other HIB's. <br /><br />

	A temporary HIB is created on each of the map nodes to hold
	the set of images it is responsible for downloading:
	<pre id="Current">
  <font id="Public">public</font> <font id="Void">void</font> map(IntWritable key, Text value, Context context) <font id="Throws">throws</font> IOException, InterruptedException
  {
    String temp_path = conf.get(<font id="StringLiteral">"downloader.outpath"</font>) + key.get() + <font id="StringLiteral">".hib.tmp"</font>;
            
    HipiImageBundle hib = <font id="New">new</font> HipiImageBundle(<font id="New">new</font> Path(temp_path), conf);
    hib.open(HipiImageBundle.FILE_MODE_WRITE, <font id="True">true</font>);
    ...
	</pre>

	Each of the Mapper's then proceeds to iterate through its set
	of URL's (contained in the value). To perform the actual
	download, the <a href="">URLConnection</a> is used to create a
	stream that can be written directly to the local HIB:
	<pre id="Current">
    String word = value.toString();
    BufferedReader reader = <font id="New">new</font> BufferedReader(<font id="New">new</font> StringReader(word));
    String uri;

    <font id="While">while</font>((uri = reader.readLine()) != <font id="Null">null</font>)            
    {
      ...
      String type = <font id="StringLiteral">""</font>;
      URLConnection conn;

      <font id="Try">try</font> {
        URL link = <font id="New">new</font> URL(uri);
	conn = link.openConnection();
	conn.connect();
	type = conn.getContentType();
      } <font id="Catch">catch</font> (Exception e)
      {
        <font id="Continue">continue</font>;
      }
      <font id="If">if</font> (type != <font id="Null">null</font> &amp;&amp; type.compareTo(<font id="StringLiteral">"image/jpeg"</font>) == <font id="IntegerLiteral">0</font>)
	hib.addImage(conn.getInputStream(), ImageType.JPEG_IMAGE);
      ...
    }
	</pre>

	Once all of the URL's have been processed, the HIB must be
	closed and a record indicating its location on the filesystem
	is sent to the Reducer:
	<pre id="Current">
  hib.close();
  context.write(<font id="New">new</font> BooleanWritable(<font id="True">true</font>), <font id="New">new</font> Text(hib.getPath().toString()));
	</pre>

	<h3>The Reducer</h3>

	The <a class="external_link" href="">Reducer's</a> job is
	relatively straighforward. It loads each of the <a
	href="">HipiImageBundles</a> (HIB) from the <a
	class="external_link" href="">Mappers</a> and merges them into
	one large HIB:
	<pre id="Current">
  <font id="Public">public</font> <font id="Void">void</font> reduce(BooleanWritable key, Iterable&lt;Text&gt; values, Context context) <font id="Throws">throws</font> IOException, InterruptedException
  {
    <font id="If">if</font>(key.get()) {
      FileSystem fileSystem = FileSystem.get(conf);
      HipiImageBundle hib = <font id="New">new</font> HipiImageBundle(<font id="New">new</font> Path(conf.get(<font id="StringLiteral">"downloader.outfile"</font>)), conf);
      hib.open(HipiImageBundle.FILE_MODE_WRITE, <font id="True">true</font>);
      <font id="For">for</font> (Text temp_string : values) {
        Path temp_path = <font id="New">new</font> Path(temp_string.toString());
	HipiImageBundle input_bundle = <font id="New">new</font> HipiImageBundle(temp_path, conf);
	hib.append(input_bundle);
                    
	Path index_path = input_bundle.getPath();
	Path data_path = <font id="New">new</font> Path(index_path.toString() + <font id="StringLiteral">".dat"</font>);
	
	context.write(<font id="New">new</font> BooleanWritable(<font id="True">true</font>), <font id="New">new</font> Text(input_bundle.getPath().toString()));
	context.progress();
      }
      hib.close();
    }
  }
	</pre>
      </div>
    </div>
    <!-- End Content -->
  </body>
</html>
