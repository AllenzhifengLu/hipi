<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="Description" content="Hadoop Image Processing Interface example for creating a HIB from a set of ,potentially Flickr, image URLs in a distributed manner." />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Distributed Downloader Example</title>
  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="../index.html">Introduction</a></li>
	<li><a href="../gettingstarted.html">Getting Started</a></li>
	<li><a href="../doc/api/index.html">Documentation</a></li>
	<li><a href="../examples.html">Examples</a></li>
	<li><a href="../downloads.html">Downloads</a></li>
	<li><a href="../about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">
      
      <h2>The HIPI Distributed Downloader</h2>
      <div class="section">
	
	The HIPI Distributed Downloader is an example program that
	creates a <a href="">HipiImageBundle</a> from a list of image
	URL&#39;s. This list can be generated by first
	&#39;scraping&#39; a website that contains organized photo
	URL's, such as <a class="external_link"
	href="http://www.flickr.com">Flickr</a> or <a
	class="external_link" href="http://images.google.com">Google
	Images</a>.
	
	<h3>Compiling the Example</h3>
	
	Before you can compile this (or any other example) you must
	configure the compiling script so that it knows where your
	Hadoop installation resides. Open up the build.xml file in the
	root directory of your HIPI installation. At the top of the
	file are two important properties that are left blank named
	<strong>hadoop.home</strong> and
	<strong>hadoop.version</strong>. Fill in the value attributes
	of these two properties with the location of your Hadoop
	installation and your Hadoop version. For instance, if you
	downloaded Hadoop 0.20.1 and unpacked it to
	/hadoop/hadoop-0.20.1, then you would have the following
	build.xml file:
	<pre id="Current">
  &#60;project basedir="." default="all"&#62;
	  
  &#60;target name="setup"&#62;
  &#60;property name="hadoop.home" value="/hadoop/hadoop-0.20.1" /&#62;
  &#60;property name="hadoop.version" value="0.20.1" /&#62;
  &#60;property name="hadoop.classpath" value="${hadoop.home}/hadoop-${hadoop.version}-core.jar" /&#62;
  &#60;property name="metadata.jar" value="3rdparty/metadata-extractor-2.3.1.jar" /&#62;
  &#60;/target&#62;
  ...
          </pre><br />
	  
	  You can compile this example by executing the following
	  command in the root directory where you unpacked HIPI:
	  <pre id="Current">
  $> ant downloader
	    </pre><br />
	    
	    This will run an ANT script that will create the file
	    examples/download.jar. If you navigate to the examples folder
	    and execute the following command, you should see the usage
	    message for the downloader program:
	    <pre id="Current">
  $> ./runDownloader.sh
	    </pre>
	    
	    <div class="important">Important Note:</div> You must be using
	    Java JDK version 1.6 or none of the HIPI examples will
	    compile.
	    
	    <h3>Running the Example</h3> 
	    
	    The downloader program takes three command line parameters,
	    which you can specify directly to the
	    <tt>runDownloader.sh</tt> script. The first parameter is the
	    Hadoop Distributed File System (HDFS) path to the list of
	    URL's to be downloaded. Each URL must reside as an ASCII
	    string on its own line in this file. The second parameter is
	    the HDFS path to the output file that will be created once the
	    program has finished. The third and final parameter specifies
	    the number of nodes that will be used to distribute the
	    downloader. This should be set in accordance with the
	    bandwidth your system can sustain. For example:
	    <pre id="Current">
  $> ./runDownloader.sh /hdfs/path/to/list.txt /hdfs/path/to/output.hib 100
	    </pre><br />
	      
	    In this case, 100 nodes will download the images specified in
	    the list.txt file and will be stored as a <a
	    href="">HipiImageBundle</a> in the output.hib file. If the
	    list of URL's contains 100,000 images, then each of the 100
	    nodes in the cluster will be responsible for downloading 1,000
	    images. In general this has proven to be a good number of
	    images per node to use.
      </div>
      
      <h2>Understanding the Downloader</h2>
      <div class="section">
	
	<img class="centered_image" src="downloader.png" width="500" alt="" />
	
	The image above depicts the natural pipeline for the downloader
	<h3>Step 1: Specifiy a list of images to collect</h3>
	We assume that there exists a well-formed list containing url's of images to download. This list should be stored in a text file with exactly one image url per line. This list can be generated by hand, by MySQL, or from a search query (e.g. google images, flickr, etc.)
	
	
	In addition to the list of images, the user will input the number of nodes to run the task. According to this input, we will divide the image set across the specified number of nodes for maximum efficiency and parallelism when downloading the images. Each node in the Map task will generate a HipiImageBundle containing all of them images it downloaded, then the Reducer will merge all the HipiImageBundles together to form one large HipiImageBundle.
	
	
	<h3>Step 2: Split URLs into groups and send each group to a Mapper</h3>
	Using the inputted list of image urls and the number of nodes used to download these images, we will equally distribute the task of downloading images to the specified number of map nodes. This allows for maximum parallelization for the downloading process. Our DownloaderInputFormat class allocates the proper number of nodes based on the user-inputted value.
	
	Once the image sets have been distributed to the various nodes, the map tasks will begin running. In other words, each map task will have a set of images to download. Each map task will iterate over these sets to download every image. The mappers retrieve the url of the image to be downloaded for all images in every set specified.
	
	
	<h3>Step 3: Download images from the internet</h3>
	We then establish a connection to the url retrieved from the database and download the image using java's URLConnection class. Once connected, we check the file type to make sure it is an image (currently we only accept JPEG images), and get an InputStream to the connection. From this point we can use the InputStream to add the image to a HipiImageBundle (detailed below). Each map task will create a HIPI Image Bundle that contains the images it has downloaded, ensuring everything will be done in parallel.
	
	
	<h3>Step 4: Store images in a HipiImageBundle</h3>
	We store images in HIPI Image Bundles in the Map phase, then merge the various HIPI Image Bundles together in the reduce phase. The results of the reduce phase is one large HIPI Image Bundle containing all of the images that have been downloaded. This way, we ensure that we achieve maximum parallelization while downloading, and also can achieve the benefits of storing images in a HIPI Image Bundle with the final result.
	HipiImageBundle provides an intuitive interface for dealing with image sets. HipiImageBundles create an index file containing the image offsets, and a data file that contains image files concatenated with each other. The diagram below demonstrates the file structure for HipiImageBundles
	
	<img class="centered_image" src="hib.png" alt="" />
	
	By storing images this way, you are able to take advantage of our HIPI framework for MapReduce tasks. For example, to check the results of the Downloader program, I ran a very simple MapReduce Program (7 lines) that was able to take the HipiImageBundle and write out the images to individual JPEG files effortlessly.
	
      </div>
    </div>
    <!-- End Content -->
  </body>
</html>
