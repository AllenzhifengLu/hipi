<html><head><title>Computing Covariance Matrix with HIPI on Hadoop</title></head><body style="line-height:160%;margin-top:24px;margin-bottom:64px">

<h1>Computing Covariance Matrix with HIPI on Hadoop</h1>

<p>This example will run through a non-trivial application based on "The Principal Components of Natural Images" by Hancock et al. Using HIPI, we run the same application on a massive data set (as opposed to 15 images in the original paper) and compare our results for the first 15 principal components of randomly sampled images. To do this, we calculate the covariance matrix for 100 random samples from each image, then perform SVD on the covariance matrix to get the principal components. Covariance is calculated according to the following formula:</p>
<img src="cov_formula.png" />
<p>Where x_i is a random sample from our image set, and x_bar is the mean of the random samples. This equation turns out to be embarassingly parallel. We will use this quality to run a MapReduce job with HIPI extremely efficiently.</p>

The general flow of the application is as follows
<ol><li>Determine location of random sample patches
<li>Calculate mean of sample patches
<li>Compute the contribution of a particular patch x_i to the Covariance Matrix: <img src="cov_partial_sum.png" />
<li>Sum the contribution of each random sample to get the Covariance Matrix.
<li>Perform SVD on the Covariance Matrix to acquire first 15 principal components (done in Matlab)
</ol>

This flow requires us to run multiple MapReduce jobs (one to compute the average, one to compute the covariance). However, our example program handles this all in one program. The user is only required to provide the input of images (a HIPI Image Bundle), the number of random samples per image, the size of the sample patch, and the location to output the covariance matrix.

<h2>Compute Average Patch</h2>
<p>To compute the average patch of an image, we use a MapReduce job to help parallelize the calculation. In the map phase, we first determine the location of the 100 random samples for a particular image (and store this data), then compute the average of these samples. We emit this average to the reduce phase, which will collect the average from every image, and take the average of those results to come up with the average patch for the entire sample. Simply put, it is an average of averages algorithm.

<h2>Compute Covariance Matrix</h2>
<p>Now that we have the average patch, we can compute the covariance according to the equation:<br/><img src="cov_formula.png" /><br />
Notice that this is a summation across each sample patch, and that the terms of the summation are independent of each other. As a result, this summation is extremely parallel. In other words, we can compute the contribution of each sample patch to the covariance matrix independently (i.e. in parallel) and sum the results up later.</p>
<p>Knowing this, we parallelize this job by image, so that each map node is responsible for the 100 random samples that correspond to an image. In the map phase, we compute the contribution of a particular patch x_i to the Covariance Matrix: <img src="cov_partial_sum.png" /> and emit these results to the reducer. In the reduce phase, we sum these terms up and result in the Covariance Matrix for our entire sample set. Parallelizing this process is extremely beneficial, as there are matrix multiplications performed in calculating each term in the summation.

<h2>Results</h2>
<p>After determining the covariance for our randomly sampled patches, we used Matlab to find the first 15 principal components. </p>
<img src="hancock_pca.png" /><br/>The first 15 principal components of 15 natural images that were randomly sampled<br/>
<img src="hipi_pca.png" width="392"  height=" 238"/><br/>The first 15 principle components of a massive image set randomly sampled, as calculated using HIPI<br/>
<p>As expected, images do not correlate perfectly because we are using far different inputs to our experiments, and our display of positive and negative values also may differ slightly. However, certain principal components are the same (1, 7, 12), are merely switched (2 and 3, 4 and 5), or show some resemblance to the original experiment but are rotated or mirrored (15). Performing a principal component analysis on a massive, unrestricted data set gives us unparalleled knowledge about images. For tasks such as these, HIPI excels.
</p>

