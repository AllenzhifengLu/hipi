<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta name="Description" content="Hadoop Image Processing Interface example for computing the principle components of natural image patches using Hadoop MapReduce." />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Principal Components of Natural Image Patches</title>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-23539446-1']);
      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
   
    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
    <style> pre.prettyprint {padding:20px 20px 0px 20px;} </style>
 
    <style type="text/css">
      .math { font-size:110%; font-weight:bold; }
    </style>
  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="../index.html">Overview</a></li>
	<li><a href="../gettingstarted.html">Getting Started</a></li>
	<li><a href="../documentation.html">Documentation</a></li>
	<li><a href="../examples.html">Tools and Examples</a></li>
	<li><a href="../downloads.html">Downloads</a></li>
	<li><a href="../contribute.html">Contribute</a></li>
	<li><a href="../about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">

      <h2 class="title">The Principal Components of Natural Images</h2>

      <div class="section">

	<!--
	The Covariance program is the main computation routine used
	when computing the Principle Components of a dataset of
	images. For information on how to generate a dataset of
	images, please see our <a href="downloader.html">Distributed
	Downloader Example</a>. If you are new to HIPI, you should
	also work through the <a href="dumphib.html">DumpHIB</a>
	example before proceeding.
	<h3>Introduction</h3>
-->

	This example program reproduces the key result in the seminal 1992 computer vision paper <a class="external_link" href="http://www.ib.cnea.gov.ar/~redneu/2013/BOOKS/Hancock_Baddeley_Smith_1992.pdf">The Principal Components of Natural Images</a> by Hancock et al. We perform the same experiment described in that paper using HIPI, which enables analyzing a much larger collection of images than the 15 used in the original paper. To understand this program, we need to first review a bit of linear algebra.<br /><br />

	Our goal is to reveal statistical properties of the pixel values in small patches of the types of images we often capture (e.g., snapshots, selfies, landscape photos, etc.). Why would this be useful? Well, this would tell us something about the structure of <i>natural images</i> and would provide a basis for certain image processing and compression algorithms. Specifically, we are interested in computing the second-order statistics, also called the <i>covariance</i>, which gives the expected value of the squared deviation across <i>all pairs of pixels</i> in a patch. It turns out that the eigenvectors of the covariance matrix reveal the <i>dominant modes of variance</i> across the set of input patches. Because these are so useful for many data analysis and compression tasks they are given the name <i>principal components</i> and this process is often called <i>Principal Component Analysis</i>. <br /><br />

	Following Hancock et al. we will be using image patches that are <i>48 x 48</i> in size. Let's assume that we have collected <b>n</b> such patches by randmoly sampling them from a set of images downloaded from the Internet. If we denote a single image patch by the vector <b>x</b> then the <i>sample covariance</i> computed from these <b>n</b> patches is given by the following formula:

      <img class="centered_image" src="../images/pca/cov_formula.png" alt="" /><br />

      where the subscript 'i' indexes the set of <b>n</b> images and <span class="math">x&#772;</span> is the average, or mean, image patch. Next, let's rewrite this expression in matrix form using <span class="math">x&#770;</span> to denote mean-centered patches (<span class="math">x<sub>i</sub> - x&#772;</span>):

      <img class="centered_image" src="../images/pca/cov_parallel.png" alt="" /><br />

      Writing the expression in this way makes it a bit easier to see how to compute the covariance matrix C using MapReduce. Our approach is to have each map task compute the product of a different submatrix (e.g., one map task will compute <span class="math">AA<sup>T</sup></span>, another will compute <span class="math">BB<sup>T</sup></span>, and so on) and the reduce task will perform the final aggregation to obtain C. Note, however, that the mean must be known ahead of time. Although there is a way to rewrite this computation that allows computing the mean and covariance with just one pass through the data, we chose this formulation for simplicity and because it illustates the use of the very handy MapReduce job cache file. Specifically, our program consists of two MapReduce jobs, the first computes the mean and the second uses this mean to compute the covariance. Our program makes use of the <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Job.html#addCacheFile(java.net.URI)">Job::addCacheFile()</a> and <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/JobContext.html#getCacheFiles()">Job::getCacheFiles()</a> methods to distribute the mean patch to all of the covariance map tasks.<br /><br />

      After the covariance matrix is computed, you can use a variety of tools like MATLAB or Python to compute its eigenvectors and obtain the principal components. The HIPI distribution includes MATLAB and Python scripts for doing this in <tt>util/readFloatImage.m</tt> and <tt>util/showCovarianceOutput.py</tt>, respectively.<br /><br />

      Below are the first 15 principal components reproduced from Hancock et al., which were computed using 20,000 patches, compared to the first 15 principal components obtained with our HIPI program for different values of <b>n</b> up to 10,000,000 patches (we used a random mix of images downloaded from <a class="external_link" href="https://www.flickr.com/services/developer/api/">Flickr</a>): 
      <table class="centered_table">
	<tr>
	  <td><img src="../images/pca/hancock.png" alt="" /></td>
	</tr>
	<tr class="caption">
	  <td>Hancock et al. - First 15 Principle Components (20,000 patches)</td>
	</tr>
	<tr>
	  <td><img src="../images/pca/hipi-100000.png" alt="" /></td>
	</tr>
	<tr class="caption">
	  <td>HIPI - First 15 Principle Components (10,000,000 patches)</td>
	</tr>
	<tr>
	  <td><img src="../images/pca/hipi-10000.png" alt="" /></td>
	</tr>
	<tr class="caption">
	  <td>HIPI - First 15 Principle Components (1,000,000 patches)</td>
	</tr>
	<tr>
	  <td><img src="../images/pca/hipi-1000.png" alt="" /></td>
	</tr>
	<tr class="caption">
	  <td>HIPI - First 15 Principle Components (100,000 patches)</td>
	</tr>
	<tr>
	  <td><img src="../images/pca/hipi-100.png" alt="" /></td>
	</tr>
	<tr class="caption">
	  <td>HIPI - First 15 Principle Components (10,000 patches)</td>
	</tr>
      </table>

      Note that the principle components computed using the HIPI program do not perfectly correlate with the ones reported by Hancock et al., although in most cases they are off by a rotation. We attribute the remaining differences to differences in some parameters like the standard deviation of the Gaussian weighting function (more on this below). These differences may also be due in part to the fact that Hancock et al. used an approximation strategy for computing the principal components which avoids explicit construction of the full covariance matrix. Also, note that the principal components become much better resolved when going from 10,000 patches to 100,000 patches, but don't noticeably improve beyond that number of patches. Finally, note that these components resemble the Fourier or Discrete Cosine Transform basis, which is a central concept in many image compression algorithms such as JPEG.

      <h3>Compiling</h3>
      
      Compile <b>covariance</b> by executing the following command in the HIPI root directory (see our <a href="../gettingstarted.html">general notes</a> on setting up HIPI on your system and using Ant for compilation):
	
      <pre class="console">
$> ant covariance
      </pre>

      <h3>Usage</h3>

      Run <b>covariance</b> by executing the following command in the HIPI root directory:

      <pre class="console">
$> hadoop jar examples/covariance.jar &#60;input HIB&#62; &#60;output directory&#62;
      </pre>
	
	There is also a convenience script located in the examples directory:

	<pre class="console">
$> cd examples
$> ./runCovariance.sh &#60;input HIB&#62; &#60;output directory&#62;
	</pre>

	<b>covariance</b> takes two arguments. The first argument is the path to a HIB on the HDFS. This HIB will provide the source of image patches used to compute the mean and covariance. The second argument is the HDFS path to the output directory that will be created once the program has finished. The resulting mean patch and covariance matrix will be stored in this directory in a raw binary format readable by the scripts in the <tt>util</tt> directory.

        <h2 class="title">How covariance works</h2>

	<b>covariance</b> consists of two phases:
	<ol>
	  <li>Compute the average (mean) of 100 randomly sampled patches taken from each image in the input HIB.</li>
	  <li>Compute the covariance matrix using the same set of 100 patches per image along with the mean computed in the previous step.</li>
	</ol>

	The output of the first phase is transfered to the second phase using the <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Job.html#addCacheFile(java.net.URI)">Job::addCacheFile()</a> and <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/JobContext.html#getCacheFiles()">Job::getCacheFiles()</a> MapReduce methods.

      <h3>The Two MapReduce Driver Classes</h3>

      Every MapReduce job must specify a driver class that configures and executes the job (e.g., specifies the various key/value object types, etc.). The driver classes for <b>covariance</b> are defined in <tt>examples/hipi/examples/covariance/Covariance.java</tt>. First, take a look at the <tt>main()</tt> and <tt>run()</tt> methods defined in the Covariance class:

      <pre class="prettyprint">
  public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Covariance(), args);
    System.exit(res);
  }

  public int run(String[] args) throws Exception {
    if (args.length !=2) {
      System.out.println("Usage: covariance &#60;input HIB&#62; &#60;output directory&#62;");
      System.exit(0);
    }
    if (!runComputeMean(args)) {
      return 1;
    }
    if (!runCovariance(args)) {
      return 1;
    }
    // Indicate success
    return 0;
  }
      </pre>

      <tt>main()</tt> is the entry point of the program and it simply calls the <tt>run()</tt> method which performs the two MapReduce jobs described above only if the first one succeeds. Because the operation of the <tt>runComputeMean()</tt> method is pretty standard so we will forego a detailed explanation here. Please see the <a href="dumphib.html">DumpHIB</a> example program to learn how to setup a standard HIPI job.<br /><br />

      The <tt>runCovariance()</tt> method is slightly more complicated and interesting. Note how it calls <a class="external_link" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Job.html#addCacheFile(java.net.URI)">Job::addCacheFile()</a> to distribute the previously computed mean patch to the covariance map tasks:

      <pre class="prettyprint">
  public boolean runCovariance(String[] args) throws Exception {
    ...
    job.addCacheFile(new URI("hdfs://" + args[1] + "/mean-output/part-r-00000"));
    ...
      </pre>

      <h3>The Mean Mapper</h3>

      Here is the <tt>map()</tt> method in the MeanMapper class along with the helper method <tt>generateMeanImage()</tt>:

      <pre class="prettyprint">
    public void map(ImageHeader key, FloatImage value, Context context) throws IOException, InterruptedException {
      if (value != null && value.getWidth() &#62; N && value.getHeight() &#62; N) {
        context.write(new IntWritable(0), generateMeanImage(value, 100, 100));
      }
    }

    // Compute the mean of (xPatchCount * yPatchCount) patches within the input image
    private FloatImage generateMeanImage(FloatImage input, int xPatchCount, int yPatchCount) {
      FloatImage mean = new FloatImage(N, N, 1);
      for (int i = 0; i &#60; xPatchCount; i++) {
        int x = (input.getWidth() - N) * i / xPatchCount;
        for (int j = 0; j &#60; yPatchCount; j++) {
          int y = (input.getHeight() - N) * j / yPatchCount;
          FloatImage patch = input.crop(x, y, N, N).convert(FloatImage.RGB2GRAY);
          mean.add(patch);
        }
      }
      mean.scale((float) (1.0 / (xPatchCount * yPatchCount)));
      return mean;
    }
  }
      </pre>

      This code makes use of several methods in the <a href="../doc/api/hipi/image/FloatImage.html">FloatImage</a> class such as <tt>crop()</tt> and <tt>convert()</tt>. The program uses a fixed patch size of <tt>N=48</tt> and a fixed 100 x 100 subsampling procedure. Also, note that the output key is always <tt>0</tt>, ensuring that all of the partial means will be sent to the same reduce task.

      <h3>The Mean Reducer</h3>

      Here is the <tt>reduce()</tt> method in the MeanReducer class:

      <pre class="prettyprint">
    public void reduce(IntWritable key, Iterable&#60;FloatImage&#62; values, Context context) 
        throws IOException, InterruptedException {
      FloatImage mean = new FloatImage(N, N, 1);
      int total = 0;
      for (FloatImage val : values) {
        mean.add(val);
        total++;
      }
      if (total &#62; 0) {
        mean.scale(1.0f / total);
        context.write(key, mean);
      }
    }
  }
      </pre>

    Since each of the partial means output by the map tasks were computed using the same number of patches, there is no need to weight these partial means differently in the final sum.

      <h3>The Covariance Mapper</h3>

      Recall that the covariance calculation requires the average image patch. This object is accessed inside the <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Mapper.html#setup(org.apache.hadoop.mapreduce.Mapper.Context)">Mapper:setup()</a> method defined in CovarianceMapper with the following lines:

      <pre class="prettyprint">
	// Access the job cache
        URI[] files = new URI[1];
        if (job.getCacheFiles() != null) {
          files = job.getCacheFiles();
        } else {
          System.err.println("Job cache files is null!");
        }

        // Read mean from previously run mean job
        Path cacheFilePath = new Path(files[0].toString());
        FSDataInputStream dis = FileSystem.get(job.getConfiguration()).open(cacheFilePath);
        dis.skip(4);
        FloatImage image = new FloatImage();
        image.readFields(dis);
        mean = image.getData();
      </pre>

      The <tt>dis.skip(4)</tt> function call is needed to discard the key (4 bytes long) that was written to the output of the MeanReducer.<br /><br />

      The <tt>map()</tt> method defined in the CovarianceMapper class uses this <tt>mean</tt> member variable to compute the sample covariance over 100 patches from within one image. In the code below, you may notice that these patches are first converted to grayscale and then Gaussian weighted during the covariance calculation:

      <pre class="prettyprint">
    public void map(ImageHeader key, FloatImage value, Context context) throws IOException, InterruptedException {
      if (value != null && value.getWidth() &#62; N && value.getHeight() &#62; N) {
        // Holds 100 patches as they are collected from the image
        float[][] patchArray = new float[100][N * N];
        // Generate mean-subtracted (whitened) and Gaussian weighted patches and stores them in patchArray
        for (int i = 0; i &#60; 10; i++) {
          int x = (value.getWidth() - N) * i / 10;
          for (int j = 0; j &#60; 10; j++) {
            int y = (value.getHeight() - N) * j / 10;
            FloatImage patch = value.crop(x, y, N, N).convert(FloatImage.RGB2GRAY);
            float[] pels = patch.getData();
            for (int k = 0; k &#60; N * N; k++) {
              // Subtract mean and weight using Gaussian mask
              patchArray[i * 10 + j][k] = (pels[k] - mean[k]) * gaussianArray[k];
            }
          }
        }
        ...
      }
    }
      </pre>

      The Gaussian weighting effectively gives higher weight to the center pixels in a patch (see <a class="external_link" href="http://www.ib.cnea.gov.ar/~redneu/2013/BOOKS/Hancock_Baddeley_Smith_1992.pdf">Hancock et al.</a> for details). Finally, the covariance is computed from the <tt>patchArray</tt>:

      <pre class="prettyprint">
        ...
        // Stores the (N^2 x N^2) covariance matrix AAt
        float[] covarianceArray = new float[N * N * N * N];
        for (int i = 0; i &#60; N * N; i++) {
          for (int j = 0; j &#60; N * N; j++) {
            covarianceArray[i * N * N + j] = 0;
            for (int k = 0; k &#60; 10; k++) {
              covarianceArray[i * N * N + j] += patchArray[k][i] * patchArray[k][j];
            }
          }
        }
        context.write(new IntWritable(0), new FloatImage(N * N, N * N, 1, covarianceArray));
      }
    }
      </pre>

      The resulting covariance matrix is represented as a <a href="../doc/api/hipi/image/FloatImage.html">FloatImage</a> and emitted to the reduce task with a call to <tt>context.write()</tt>.

      <h3>The Covariance Reducer</h3>

      The <tt>reduce()</tt> method in the CovarianceReducer class combines the partial covariances emitted by the map tasks into the final covariance. As with the reducer method for the mean program, each map task processes the same number of patches so there is no need to weight the partial covariances differently. The output of <tt>reduce</tt> is simply the sum of these input covariances, again represented as a FloatImage:

      <pre class="prettyprint">
    public void reduce(IntWritable key, Iterable&#60;FloatImage&#62; values, Context context) throws IOException, InterruptedException {
      // Aggregate sub-matrices in full covariance calculation
      FloatImage cov = new FloatImage(N * N, N * N, 1);
      for (FloatImage val : values) {
        cov.add(val);
      }
      context.write(key, cov);
    }
  }
      </pre>
    
    The output of <b>covariance</b> consists of two files named <tt>&#60;output directory&#62;/mean-output/part-r-00000</tt> and <tt>&#60;output directory&#62;/covariance-output/part-r-00000</tt>. These binary files can be viewed using a simple MATLAB or Python program. The HIPI distribution includes sample programs in each of these languages. Here is the MATLAB function that reads a FloatImage into a MATLAB matrix:

      <pre class="prettyprint">
  function [ y ] = readFloatImage( file )
    fid = fopen(file);
    if fid ~= 0
      fread(fid, 1, 'int32', 0, 'b');
      width = fread(fid, 1, 'int32', 0, 'b');
      height = fread(fid, 1, 'int32', 0, 'b');
      band = fread(fid, 1, 'int32', 0, 'b');
      y = reshape(fread(fid, width * height * band, 'float32', 0, 'b'), width, height, band);
    else
      y = [];
    end
  end
      </pre>

      An extended version of this function was used to generate the result images displayed above:

      <pre class="prettyprint">
  function [ y ] = readCovarianceOutputAndGenerateImages( file )
    fid = fopen(file);
    if fid ~= 0
      fread(fid, 1, 'int32', 0, 'b');
      width = fread(fid, 1, 'int32', 0, 'b');
      height = fread(fid, 1, 'int32', 0, 'b');
      band = fread(fid, 1, 'int32', 0, 'b');
      y = reshape(fread(fid, width * height * band, 'float32', 0, 'b'), width, height, band);
      [V,d] = princomp(y);
      for n = 1:15
        column_data = V(:,n);
        I = mat2gray(vec2mat(column_data, 48));
        figure, imshow(I)
      end
    else
      y = [];
    end
  end
   </pre>     

      If you'd like to learn more about Principal Component Analysis, Singular Value Decomposition, and related topics from linear algebra, we recommend the excellent book <a href="http://www.amazon.com/Computations-Hopkins-Studies-Mathematical-Sciences/dp/0801854148">Matrix Computations</a> by Gene Golub.

    </div>
    
  </div>
  <!-- End Content -->
  </body>
</html>
