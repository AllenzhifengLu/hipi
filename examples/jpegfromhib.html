<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta name="Description" content="Hadoop Image Processing Interface example for creating a set of JPEG images from a HipiImageBundle." />
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: JpegFromHib Example</title>

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-23539446-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>

</head>

<body>
    
    <div class="header">
        <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    
    <div class="navigation_menu">
        <ul>
            <li><a href="../index.html">Introduction</a>
            </li>
            <li><a href="../gettingstarted.html">Getting Started</a>
            </li>
            <li><a href="../documentation.html">Documentation</a>
            </li>
            <li><a href="../examples.html">Examples</a>
            </li>
            <li><a href="../downloads.html">Downloads</a>
            </li>
            <li><a href="../about.html">About</a>
            </li>
        </ul>
    </div>

    <!-- Begin Content -->
    <div class="content">
        <h2>JpegFromHib</h2>
            <div class="section">
                JpegFromHib is an example program that creates a set of jpeg images from an input 
                    <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a> (HIB).
                    <h3>Compiling the Example</h3>
                      Before you can compile this (or any other example) you must configure the compiling script so that it knows where your Hadoop 
                      installation resides. Open up the build.xml file in the root directory of your HIPI installation. At the top of this file are 
                      three important properties named <strong>hadoop.home</strong>,<strong>hadoop.version</strong>, and <strong>hadoop.classpath</strong>. 
                      Fill in the value attributes of these three properties with the location of your Hadoop installation and your Hadoop version. 
                      For instance, if you downloaded Hadoop 2.5.1 and unpacked it to /opt/hadoop-2.5.1/share/hadoop, then you would have the following 
                      build.xml file:
              
                    <pre id="Current">
                      &#60;project basedir="." default="all"&#62;

                      &#60;target name="setup"&#62;
                      &#60;property name="hadoop.version" value="2.5.1" /&#62;
                      &#60;property name="hadoop.home" value="/opt/hadoop-${hadoop.version}/share/hadoop" /&#62;
                      &#60;property name="hadoop.classpath" value="${hadoop.home}/common/hadoop-common-${hadoop.version}.jar:
                      ${hadoop.home}/mapreduce/hadoop-mapreduce-client-core-${hadoop.version}.jar:
                      ${hadoop.home}/common/lib/commons-cli-1.2.jar:${hadoop.home}/hdfs/hadoop-hdfs-${hadoop.version}.jar:
                      ${hadoop.home}/common/hadoop-nfs-${hadoop.version}.jar" /&#62;
                      ...
                    </pre>

                    <div class="important">Important Note:</div>
                      Hadoop 2.5.1 requires five jars on its classpath (hadoop-common, hadoop-mapreduce-client-core, 
                      commons-cli, hadoop-hdfs, and hadoop-nfs). Ensure that these jar files are referenced in the hadoop.classpath property.</br>
                    </br>

                    You can compile this example by executing the following command in the root directory where you unpacked HIPI:
                    <pre id="Current">
                      $> ant jpegfromhib
                    </pre>

                    <div class="important">Important Note:</div>
                      You must be using Java JDK version 1.8 in order to ensure that HIPI will compile correctly. Although earlier versions
                      <em>may</em> work, we have not fully tested them.
                    </br>
                  
                <h3>Running the Example</h3> 
                    We have provided a script in the <tt>examples</tt> directory of the HIPI installation that will automatically compile the 
                    jpegfromhib program, upload the resulting jar to the  <a class="external_link" 
                    href="http://hadoop.apache.org/docs/r2.5.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">
                    Hadoop Distributed Filesystem (HDFS) </a>, and run the jpegfromhib job.
                    <br /><br />
    
                    The jpegfromhib program takes two command line parameters, which you can specify directly to the
                    <tt>runJpegFromHib.sh</tt> script. The first parameter is the HDFS path to the HIB being referenced. The second parameter is 
                    the HDFS path to the output directory that will be created once the program has finished. The resulting jpeg files will be 
                    stored in this directory. For example:
                    <pre id="Current">
                      $> ./runJpegFromHib.sh /hdfs/path/to/input.hib /hdfs/path/to/outputDirectory
                    </pre>
            </div>

          <h2>Understanding JpegFromHib</h2>
            <div class="section">
              <h3>JpegFromHibRecordReader</h3>
                This class makes use of an internal <a href="../doc/api/hipi/imagebundle/HipiImageBundle.FileReader.html">HipiImageBundle.FileReader</a>, 
                which contains the functionality needed to parse through a 
                <a href="../doc/api/hipi/imagebundle/HipiImageBundle.FileReader.html">HipiImageBundle</a> (HIB). 
                This record reader will draw raw bytes from the HIB and encapsulate them in a Hadoop 
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/io/BytesWritable.html">BytesWritable</a> object 
                which will be passed into the mapper:
                <pre>
                  @Override
                  <font id="Public">public</font> <font id="Void">void</font> initialize(InputSplit split, TaskAttemptContext context) <font id="Throws">throws</font> IOException,
                      InterruptedException {
                    ...
                    reader = <font id="New">new</font> HipiImageBundle.FileReader(fs, path, conf, bundleSplit.getStart(), 
                      bundleSplit.getStart() + bundleSplit.getLength() - 1);
                  }
                </pre>
                The initialize() method creates a HipiImageBundle.FileReader and encapsulates it within the JpegFromHibRecordReader class. 
                This FileReader is configured to reference the HIB in question.
                <pre>
                  @Override
                  <font id="Public">public</font> BytesWritable getCurrentValue() <font id="Throws">throws</font> IOException, InterruptedException {
                    <font id="Return">return</font> <font id="New">new</font> BytesWritable(reader.getRawBytes());
                  }
                </pre>
                This method illustrates how JpegFromHibRecordReader makes use of the previously initialized fileReader to return data from the Hib.

              <h3>JpegFromHibInputFormat</h3>
                This class contains a method which will return a JpegFromHibRecordReader. It also contains a method which returns a List of 
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a> objects. 
                According to the Hadoop 2.5.1 API Documentation:
                  <br/><br/><q>InputSplit represents the data to be processed by an individual Mapper. Typically, it presents a byte-oriented 
                  view on the input and is the responsibility of RecordReader of the job to process this and present a record-oriented view.</q>
                  <br/><br/>
                By this definition, JpegFromHibInputFormat creates and provides data to the JpegFromHibRecordReader, which passes data into the MapReduce Job.
              <h3>JpegFromHibMapper</h3>
                The mapper accepts as input a
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/io/BytesWritable.html">BytesWritable</a> 
                object which is passed to it by the previously mentioned JpegFromHibInputFormat class. Here is the mapper method in its entirety:
                <pre>
                  @Override
                  <font id="Public">public</font> <font id="Void">void</font> map(NullWritable key, BytesWritable value, Context context) <font id="Throws">throws</font> IOException,
                      InterruptedException {
                    <font id="If">if</font> (value == null) {
                      <font id="Return">return</font>;
                    }
                    <font id="String">String</font> hashval = ByteUtils.asHex(value.getBytes());
                    Path outpath = new Path(path + <font id="StringLiteral">"/"</font> + hashval + <font id="StringLiteral">".jpg"</font>);
                    FSDataOutputStream os = fileSystem.create(outpath);
                    os.write(value.getBytes());
                    os.flush();
                    os.close();
                    context.write(new BooleanWritable(true), new Text(hashval));
                  }
                </pre>
                After receiving a
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/io/BytesWritable.html">BytesWritable</a>
                object, the mapper makes use of HIPI's <a href="../doc/api/hipi/util/ByteUtils.html">ByteUtils</a> to generate a hash string from the internal 
                bytes of the input (this provides a unique name for each output image). A
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/fs/FSDataOutputStream.html">FSDataOutputStream</a> 
                instance is then used to write a file to the previously defined outpath. Every time this method is called, a single .jpg image is generated.

              <h3>Reducer</h3>
                The reducer in this example is trivial, since there is no consolidation required for the jpeg images created by the mapper step. This 
                example makes use of Hadoop's base
                <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a> class - 
                a reducer is not implemented by this example.
        </div>
    </div>
    <!-- End Content -->
</body>

</html>
