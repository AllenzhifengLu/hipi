<!DOCTYPE html">
<html>
  <head>
    <meta name="Description" content="Hadoop Image Processing Interface example for creating a HipiImageBundle from a set of image URLs (potentially making use of Flickr)." />
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Distributed Downloader</title>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-23539446-1']);
      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>

    <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
    <style> pre.prettyprint {padding:20px 20px 0px 20px;} </style>

  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="../index.html">Overview</a></li>
	<li><a href="../gettingstarted.html">Getting Started</a></li>
	<li><a href="../documentation.html">Documentation</a></li>
	<li><a href="../examples.html">Tools and Examples</a></li>
	<li><a href="../downloads.html">Downloads</a></li>
	<li><a href="../contribute.html">Contribute</a></li>
	<li><a href="../about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">
      
      <h2 class="title">DistributedDownloader</h2>

      <div class="section">
	
	The DistributedDownloader is a MapReduce/HIPI program that creates a <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a> (HIB) from a set of images downloaded over the Internet. This set is specified as a list of URLs and it can be generated in many different ways. For example, you might create a list of image URLs by scraping a website that contains photo collections like <a class="external_link" href="https://www.flickr.com/services/developer/api/">Flickr</a>, <a class="external_link" href="https://developers.google.com/custom-search/json-api/v1/overview">Google Images</a>, <a class="external_link" href="http://www.bing.com/dev/en-us/dev-center">Bing</a>, or <a class="external_link" href="https://instagram.com/developer/">Instagram</a>.
	
	<h3>Compiling</h3>

	Compile <b>downloader</b> by executing the following command in the HIPI root directory (see our <a href="../gettingstarted.html">general notes</a> on setting up HIPI on your system and using Ant for compilation):
	
	<pre class="console">
$> ant downloader
	</pre>
	    
	<h3>Usage</h3>
	 
	Run <b>downloader</b> by executing the following command in the HIPI root directory:

	<pre class="console">
$> hadoop jar examples/downloader.jar &#60;input text file with list of URLs&#62; &#60;output HIB&#62; &#60;number of download nodes&#62;
	</pre>
	
	There is also a convenience script located in the examples directory:

	<pre class="console">
$> cd examples
$> ./runDownloader.sh &#60;input text file with list of URLs&#62; &#60;output HIB&#62; &#60;number of download nodes&#62;
	</pre>

	<b>downloader</b> takes three arguments. The first argument is the path to an input text file located on the HDFS. This file should contain a list of valid ASCII URLs to images (either JPG or PNG), each URL on a separate line. The second argument is the HDFS path to the output HIB that will contain this set of images once they are downloaded. The third argument is the number of nodes on your Hadoop cluster that you wish to use to download the images. Specifying more than one download node causes this task to be distributed over multiple machines and may speed up the process.<br /><br />

	<h3>Example</h3>

	We will walk you through an example using the sample test images provided in the HIPI distribution. The commands below assume you are in the HIPI root directory. The first step is to copy the text file with the list of image URLs to the HDFS:

	<pre class="console">
$> hadoop fs -copyFromLocal examples/testimages.txt testimages.txt
	</pre>

	To make sure things worked as expected, we can inspect the contents of the text file now on the HDFS using the <tt>hadoop fs -cat</tt> command:

	<pre class="console">
$> hadoop fs -cat testimages.txt
http://hipi.cs.virginia.edu/examples/testimages/01.jpg
http://hipi.cs.virginia.edu/examples/testimages/02.jpg
http://hipi.cs.virginia.edu/examples/testimages/03.jpg
http://hipi.cs.virginia.edu/examples/testimages/04.jpg
http://hipi.cs.virginia.edu/examples/testimages/05.jpg
http://hipi.cs.virginia.edu/examples/testimages/06.jpg
http://hipi.cs.virginia.edu/examples/testimages/07.jpg
http://hipi.cs.virginia.edu/examples/testimages/08.jpg
http://hipi.cs.virginia.edu/examples/testimages/09.jpg
http://hipi.cs.virginia.edu/examples/testimages/10.jpg
http://hipi.cs.virginia.edu/examples/testimages/11.jpg
http://hipi.cs.virginia.edu/examples/testimages/12.png
	</pre>
	
	Note that <tt>testimages.txt</tt> contains 12 URLs to images located on the HIPI project website (these images are all in the public domain).<br /><br />

	Next, run the <b>downloader</b>:

	<pre class="console">
$> hadoop jar examples/downloader.jar testimages.txt testimages.hib 10
...
Downloading http://hipi.cs.virginia.edu/examples/testimages/01.jpg
> Took 0.467 seconds
Downloading http://hipi.cs.virginia.edu/examples/testimages/02.jpg
> Took 0.443 seconds
...
	</pre>

	In this case, we specified 10 download nodes. If we were using a cluster with at least 10 nodes this would cause the download operation to be distributed over this many nodes.  After <b>downloader</b> finishes, we verify that it created the desired HIB on the HDFS:

	<pre class="console">
$> hadoop fs -ls
Found 5 items
-rw-r--r--   1 user group        140 2015-03-12 12:15 testimages.hib
-rw-r--r--   1 user group   23774066 2015-03-12 12:15 testimages.hib.dat
drwxr-xr-x   - user group          0 2015-03-12 12:15 testimages.hib_output
-rw-r--r--   1 user group        605 2015-03-12 12:07 testimages.txt
	</pre>

	You can use tools like <a href="../examples/dumphib.html"><b>dumphib</b></a> and <a href="../examples/jpegfromhib.html"><b>JpegFromHib</b></a> to do a deeper inspection of the <tt>testimages.hib</tt> and <tt>testimages.hib.dat</tt> files.
      
	<h2 class="title">How downloader works</h2>
	
	<b>downloader</b> attempts to download a list of images located at URLs that are specified in an input ASCII text file with one URL per line. Because downloading a large set of images from a single computer can be slow and because typical cluster setups can support more bandwidth than an individual node, the downloader is designed to distribute this task to multiple nodes in a Hadoop cluster. In the following discussion, we assume that <b>downloader</b> is attempting to download <strong>k</strong> images using <strong>n</strong> download nodes and that the underlying Hadoop cluster is comprised of <strong>m</strong> compute nodes. Note that the number of download nodes requested by the user <strong>n</strong> is not necessarily equal to the number of compute nodes <strong>m</strong> though one would not expect any speed improvements after <strong>n</strong> exceeds <strong>m</strong> (i.e., once full cluster utilization is achieved). <br /><br />

	To begin, let's study the class DownloaderInputFormat located in <tt>examples/hipi/examples/downloader/DownloaderInputFormat.java</tt>. This class extends the <a class="external_link" href="https://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html">FileInputFormat</a> Hadoop class and is responsible for parsing the input text file and initiating the map tasks that download the images. In particular, the <tt>getSplits()</tt> method in this class causes <strong>n</strong> map tasks to each download (<strong>k</strong> / <strong>n</strong>) images into separate <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a> (HIBs). After these map tasks have completed, a single reduce task merges these HIBs into one HIB located at the output path specified by the user. This diagram illustrates the process:

	<img class="centered_image" src="downloader.png" width="500" alt="" />
	
	<h3>DownloaderInputFormat class</h3>

	Under normal operation, Hadoop attempts to execute each map task on whichever compute node in the cluster is &#39;closest&#39; to the input data for that task (keep in mind that the machines in a Hadoop cluster share the task of managing the distributed file system and performing distributed MapReduce computations). In the case of <b>downloader</b>, this is problematic because the input data is a small text file with a list of URLs, which will most likely be stored on a single physical machine in the cluster. Without taking steps to avoid the default behavior, Hadoop would download all of the images in serial using this one machine even if the cluster contained many nodes with separate fast connections to the Internet. In other words, Hadoop must be coerced into behaving differently in this case to fully utilize the cluster. <br /><br />

	<span class="important">Important Note:</span> This discussion is more related to how Hadoop works than HIPI, but you may still find it informative. If you're not familiar with the basics of how Hadoop works you may want to first read <a href="http://hadoop.apache.org/docs/stable/">some documentation</a>.<br /><br />

	In order to cause Hadoop to spawn <strong>n</strong> map tasks that each run on different nodes in the cluster (bear in mind this is only possible if <strong>m</strong> is greater than or equal to <strong>n</strong>), the <tt>getSplits()</tt> method in DownloaderInputFormat creates <strong>n</strong> temporary files on the HDFS. Due to Hadoop's effort to uniformly distribute data over the cluster, each new file has a high probability of being created on a different machine. Based on these observations, our process of spawning <strong>n</strong> map tasks on different machines is as follows:<br /><br />

	1. Create up to <strong>2n</strong> temporary files on the HDFS in the pursuit of identifying <strong>n</strong> unique compute nodes:

	<pre class="prettyprint">
  // Use a default value of 10 if 'downloader.nodes' is not explicitly set
  int numDownloadNodes = conf.getInt("downloader.nodes", 10);

  // Initialize list to store unique nodes in cluster
  ArrayList&#60;String&#62; uniqueNodes = new ArrayList&#60;String&#62;(0);

  // Initialize list to store InputSplits
  List&#60;InputSplit&#62; splits = new ArrayList&#60;InputSplit&#62;();

  // Create stub for temporary HIB files
  FileSystem fileSystem = FileSystem.get(conf);
  String tempOutputPath = conf.get("downloader.outpath") + "_tmp";
  Path tempOutputDir = new Path(tempOutputPath);

  // Ensure clean temporary directory
  if (fileSystem.exists(tempOutputDir)) {
    fileSystem.delete(tempOutputDir, true);
  }
  fileSystem.mkdirs(tempOutputDir);

  // Search for up to numDownloadNodes unique nodes on the cluster
  int i = 0;
  while (uniqueNodes.size() &#60; numDownloadNodes && i &#60; 2*numDownloadNodes) {

    // Create temporary file
    String tempFileString = tempOutputPath + "/" + i;
    Path tempFile = new Path(tempFileString);
    FSDataOutputStream os = fileSystem.create(tempFile);
    os.write(i);
    os.close();
	</pre>

	2. Determine which cluster node(s) each temporary file is stored on:

	<pre class="prettyprint">
    FileStatus match = fileSystem.getFileStatus(tempFile);
    long length = match.getLen();
    BlockLocation[] blocks = fileSystem.getFileBlockLocations(match, 0, length);
	</pre>

	3. Ensure this node is not already in the current list of compute nodes, otherwise try again:

	<pre class="prettyprint">
    // Check if the first node used to store this temporary file is not yet on our list
    boolean save = true;
    for (int j=0; j&#60;uniqueNodes.size(); j++) {
      if (blocks[0].getHosts()[0].compareTo(uniqueNodes.get(j)) == 0) {
        save = false;
        System.out.println("Repeated host: " + i);
        break;
      }
    }
	</pre>

	4. If the node is new, save it in the list of unique nodes and repeat until the list contains <strong>n</strong> nodes or we have created <strong>2n</strong> temporary files, whichever comes first:
	
	<pre class="prettyprint">
    // If unique, add it to list of unique nodes
    if (save) {
      uniqueNodes.add(blocks[0].getHosts()[0]);
      System.out.println("Found unique host: " + i);
    }
    i++;
  } // while( hosts.size() &#60; nodes && i &#60; 2*nodes)
	</pre>

	After the list of unique hosts has been created, the next step is to generate a <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/lib/input/FileSplit.html">FileSplit</a> object for each map task. The FileSplit class is used in a slightly unorthodox way here in order to provide each map task with the subset of the URL list that it will be responsible for downloading. The constructor for the FileSplit class normally takes four arguments: the path to the file to split, the starting byte offset for the split, the byte length of the split, and the list of hosts where the split resides. Instead of following this convention, we replace the start offset and length parameters with the line offset and number of lines in the input image URL list that are to be processed by each map task:

	<pre class="prettyprint">
  // Determine download schedule (number of images per node)
  int span = (int) Math.ceil((float)numImages / (float)uniqueNodes.size());
  int last = numImages - span * (uniqueNodes.size() - 1);

  if (uniqueNodes.size() &#62; 1) {
    System.out.println("First " + (uniqueNodes.size() - 1) + " nodes will each download " + span + " images");
    System.out.println("Last node will download " + last + " images");
  } else {
    System.out.println("Single node will download " + last + " images");
  }

  // Produce file splits according to download schedule
  FileSplit[] f = new FileSplit[uniqueNodes.size()];
  for (int j = 0; j &#60; f.length; j++) {
    String[] node = new String[1];
    node[0] = uniqueNodes.get(j);
    if (j &#60; f.length - 1) {
      splits.add(new FileSplit(path, (j * span), span, node));
    } else {
      splits.add(new FileSplit(path, (j * span), last, node));
    }
  }
	</pre>

	At this point, the DownloaderInputFormat class has produced FileSplits that divide the list of image URLs across the map tasks.

	<h3>DownloaderRecordReader class</h3>

	The Hadoop <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a> class is responsible for emitting a set of key/value pairs (also known as records) for each <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a> object that it receives from the <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/InputFormat.html">InputFormat</a> class. This is accomplished by having the map tasks repeatedly call the methods <tt>RecordReader::nextKeyValue()</tt>, <tt>RecordReader::getCurrentKey()</tt>, and <tt>RecordReader::getCurrentValue()</tt>. <br /><br />

	In the case of <b>downloader</b>, the DownloaderRecordReader is responsible for sending to each map task the set of lines in the input list of image URLs that it is responsible for downloading. As mentioned above, the DownloaderInputFormat class uses the <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/lib/input/FileSplit.html">FileSplit</a> object to hold this information. The <tt>initialize()</tt> method in DownloaderRecordReader is where these InputSplit objects are parsed to determine the subset of image URLs for each map task:

	<pre class="prettyprint">
  public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {

    // Obtain path to input list of image URLs
    FileSplit fileSplit = (FileSplit)split;
    Path path = fileSplit.getPath();
    FileSystem fileSystem = path.getFileSystem(context.getConfiguration());

    // Note the start and length fields in the FileSplit object are being used to convey a
    // range of lines in the input list of image URLs
    startLine = fileSplit.getStart();
    long numLines = fileSplit.getLength();

    // Flag to enable emitting only one key/value pair
    singletonEmit = false;

    // Created buffered reader for input list of image URLs
    BufferedReader reader = new BufferedReader(new InputStreamReader(fileSystem.open(path)));

    // Advance reader to startLine
    int i = 0;
    while (i &#60; startLine && reader.readLine() != null) {
      i++;
    }

    // Build numLines length list of image URLs delimited by newline character \n
    urls = "";
    String line;
    for (i = 0; i &#60; numLines && (line = reader.readLine()) != null; i++) {
      urls += line + '\n';
    }

    reader.close();
  }
	</pre>

	And, finally, DownloadRecordReader must also implement the functions that are called by the map tasks to obtain their input key/value pairs. In this case, there is only one record whose key and value are the starting line number and the list of URLs respectively:
	
	<pre class="prettyprint">
  public IntWritable getCurrentKey() throws IOException, InterruptedException {
    return new IntWritable((int)startLine);
  }

  public Text getCurrentValue() throws IOException, InterruptedException {
    return new Text(urls);
  }

  public boolean nextKeyValue() throws IOException, InterruptedException {
    if (singletonEmit == false) {
      singletonEmit = true;
      return true;
    } else {
      return false;
    }
  }
	</pre>

	Note that <tt>nextKeyValue()</tt> will return <tt>true</tt> only once and so one record (and one associated map task) will be generated for each InputSplit.

	<h3>DownloaderMapper class</h3>

	The <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a>
	emits records to the DownloaderMapper where the keys are the starting line in the input list of
	URLs and the values are the URLs to be downloaded (a single Text object delimited by newline characters). Within the <tt>map()</tt> method in DownloaderMapper, the task is to download
	this list of images to a <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a> (HIB). These HIBs are subsequently concatenated into a single HIB in the reduce task.<br /><br /> 

	First, a temporary HIB is created to hold the set of images downloaded by the map task:

	<pre class="prettyprint">
   public void map(IntWritable key, Text value, Context context) throws IOException, InterruptedException {
      // Create path for temporary HIB file
      String tempPath = conf.get("downloader.outpath") + key.get() + ".hib.tmp";
      HipiImageBundle hib = new HipiImageBundle(new Path(tempPath), conf);
      hib.open(HipiImageBundle.FILE_MODE_WRITE, true);
      ...
	</pre>

	Next, the <tt>map()</tt> method iterates over the list of URLs (contained in <tt>value</tt>). To perform the actual download, the <a class="external_link" href="http://docs.oracle.com/javase/7/docs/api/java/net/URLConnection.html">URLConnection</a> is used to create a stream that can be written directly to the HDFS:

	<pre class="prettyprint">
      BufferedReader reader = new BufferedReader(new StringReader(value.toString()));
      String uri;
      ...
      // Iterate through URLs
      while ((uri = reader.readLine()) != null) {
         ...
          String type = "";
          URLConnection conn;

	  // Attempt to download image at URL using java.net
          try {
            URL link = new URL(uri);
            System.err.println("Downloading " + link.toString());
            conn = link.openConnection();
            conn.connect();
            type = conn.getContentType();
          } catch (Exception e) {
            System.err.println("Connection error while trying to download: " + uri);
            continue;
          }
          ...
	  // Check that image format is supported, header is parsable, and add to HIB if so
          if (type != null && (type.compareTo("image/jpeg") == 0 || type.compareTo("image/png") == 0)) {
              ...
	      // Add image to HIB
	      hib.addImage(bis, type.compareTo("image/jpeg") == 0 ? ImageType.JPEG_IMAGE : ImageType.PNG_IMAGE);
	      System.err.println("Added to HIB: " + uri);
	    }
            ...
        }
	</pre>

	After all of the URLs have been processed, the HIB is finalized and a record indicating its location on the filesystem is sent to the reduce task:
	<pre class="prettyprint">
	// Output key/value pair to reduce layer consisting of boolean and path to HIB
        context.write(new BooleanWritable(true), new Text(hib.getPath().toString()));
	// Cleanup
        reader.close();
        hib.close();
	</pre>

	<h3>DownloaderReducer class</h3>

	The DownloaderReducer class extends the Hadoop <a class="external_link" href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a> class and implements the <tt>reduce()</tt> method. In this case, it's job is pretty straightforward: it simply appends each of the HIBs produced by the map tasks into a single HIB at the path specified by the user:

	<pre class="prettyprint">
    public void reduce(BooleanWritable key, Iterable<Text> values, Context context) 
        throws IOException, InterruptedException {
      
      if (key.get()) {

	// Get path to output HIB
        FileSystem fileSystem = FileSystem.get(conf);
        Path outputHibPath = new Path(conf.get("downloader.outfile"));

	// Create HIB for writing
        HipiImageBundle hib = new HipiImageBundle(outputHibPath, conf);
        hib.open(HipiImageBundle.FILE_MODE_WRITE, true);

	// Iterate over the temporary HIB files created by map tasks
        for (Text tempString : values) {

	  // Open the temporary HIB file
          Path tempPath = new Path(tempString.toString());
          HipiImageBundle inputBundle = new HipiImageBundle(tempPath, conf);

	  // Append temporary HIB file to output HIB (this is fast)
          hib.append(inputBundle);

	  // Remove temporary HIB (both .hib and .hib.dat files)
          Path indexPath = inputBundle.getPath();
          Path dataPath = new Path(indexPath.toString() + ".dat");
          fileSystem.delete(indexPath, false);
          fileSystem.delete(dataPath, false);

	  // Emit output key/value pair indicating temporary HIB has been processed
          Text outputPath = new Text(inputBundle.getPath().toString());
          context.write(new BooleanWritable(true), outputPath);
          context.progress();
        }

	// Finalize output HIB
        hib.close();
      }
	</pre>

	And that's that! We encourage you to play around with these examples and consider using them as starting points for your own MapReduce/HIPI programs.

	<h3>Next</h3>

	Learn about the <a href="../examples/jpegfromhib.html">JpegFromHib example program</a> that extracts the images in a HIB as individual JPEGs.

      </div>
    </div>
    <!-- End Content -->
  </body>
</html>
