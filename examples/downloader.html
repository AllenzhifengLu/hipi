<!DOCTYPE html">
<html>
  <head>
    <meta name="Description" content="Hadoop Image Processing Interface example for creating a HipiImageBundle from a set of image URLs (potentially making use of Flickr)." />
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="../include/main.css" />
    <link rel="stylesheet" type="text/css" href="../include/javasyntax.css" />
    <title>HIPI - Hadoop Image Processing Interface :: Distributed Downloader</title>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-23539446-1']);
      _gaq.push(['_trackPageview']);
  
  (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
  
    </script>
  
  </head>
  
  <body>
    <div class="header">
      <h1>HIPI - Hadoop Image Processing Framework</h1>
    </div>
    <div class="navigation_menu">
      <ul>
	<li><a href="../index.html">Introduction</a></li>
	<li><a href="../gettingstarted.html">Getting Started</a></li>
	<li><a href="../documentation.html">Documentation</a></li>
	<li><a href="../examples.html">Tools and Examples</a></li>
	<li><a href="../downloads.html">Downloads</a></li>
	<li><a href="../about.html">About</a></li>
      </ul>
    </div>
    
    <!-- Begin Content -->
    <div class="content">
      
      <h2 class="title">The HIPI Distributed Downloader</h2>

      <div class="section">
	
	The HIPI Distributed Downloader is a MapReduce/HIPI program that creates a <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html"><tt>HipiImageBundle</tt></a> (HIB) from a list of image URLs. This list can be generated in many different ways. For example, you might create a list of image URLs by scraping a website that contains photo collections like <a class="external_link" href="https://www.flickr.com/services/developer/api/">Flickr</a>, <a class="external_link" href="https://developers.google.com/custom-search/json-api/v1/overview">Google Images</a>, or <a class="external_link" href="http://www.bing.com/dev/en-us/dev-center">Bing</a>.
	
	<h3>Compiling</h3>

	Compile <b>downloader</b> by executing the following command in the HIPI root directory (see our <a href="../gettingstarted.html">general notes</a> on setting up HIPI on your system and using Ant for compilation):
	
	<pre id="Current">
   $> ant downloader
	</pre>
	    
	<h3>Usage</h3>
	 
	Run <b>downloader</b> by executing the following command in the HIPI root directory:

	<pre id="Current">
   $> hadoop jar examples/downloader.jar &#60;input text file with list of URLs&#62; &#60;output HIB&#62; &#60;number of download nodes&#62;
	</pre>
	
	This program requires three arguments. The first argument is the path to the input text file located on the HDFS. This text file should contain a list of valid ASCII URLs to images (either JPG or PNG). The second argument is the HDFS path to the output HIB that will contain this set of images once they are downloaded. The third argument is the number of nodes (technically map tasks) that you wish to use to download the images. Specifying more than one download node causes this task to be distributed over multiple machines and may speed up the process.<br /><br />

	We will walk you through an example using the sample test images provided in the HIPI distribution. The commands below assume you are in the HIPI root directory. The first step is to copy the text file with the list of image URLs to the HDFS:

	<pre id="Current">
   $> hadoop fs -copyFromLocal examples/testimages/testimages.txt testimages.txt
	</pre>

	To make sure things worked as expected, we can inspect the contents of the text file now on the HDFS using the <tt>hadoop fs -cat</tt> command:

	<pre id="Current">
   $> hadoop fs -cat testimages.txt
   http://hipi.cs.virginia.edu/examples/testimages/01.jpg
   http://hipi.cs.virginia.edu/examples/testimages/02.jpg
   http://hipi.cs.virginia.edu/examples/testimages/03.jpg
   http://hipi.cs.virginia.edu/examples/testimages/04.jpg
   http://hipi.cs.virginia.edu/examples/testimages/05.jpg
   http://hipi.cs.virginia.edu/examples/testimages/06.jpg
   http://hipi.cs.virginia.edu/examples/testimages/07.jpg
   http://hipi.cs.virginia.edu/examples/testimages/08.jpg
   http://hipi.cs.virginia.edu/examples/testimages/09.jpg
   http://hipi.cs.virginia.edu/examples/testimages/10.jpg
   http://hipi.cs.virginia.edu/examples/testimages/11.jpg	
	</pre>
	
	Note that <tt>testimages.txt</tt> lists 11 URLs to images located on the main HIPI website (these images are all in the public domain).<br /><br />

	Next, run the <b>downloader</b>:

	<pre id="Current">
   $> hadoop jar examples/downloader.jar testimages.txt testimages.hib 10
   ...
   Downloading http://hipi.cs.virginia.edu/examples/testimages/01.jpg
   > Took 0.467 seconds
   Downloading http://hipi.cs.virginia.edu/examples/testimages/02.jpg
   > Took 0.443 seconds
   ...
	</pre>

	In this case, we specified 10 download nodes. If we were using a cluster with more than 10 nodes this would cause the download operation to be distributed over this many nodes.  After <b>downloader</b> finishes, we verify that it created the HIB on the HDFS:

	<pre id="Current">
   $> hadoop fs -ls
   Found 5 items
   -rw-r--r--   1 user group        140 2015-03-12 12:15 testimages.hib
   -rw-r--r--   1 user group   23774066 2015-03-12 12:15 testimages.hib.dat
   drwxr-xr-x   - user group          0 2015-03-12 12:15 testimages.hib_output
   -rw-r--r--   1 user group        605 2015-03-12 12:07 testimages.txt
	</pre>

	You can use other tools like <a href="../examples/dumphib.html"><b>dumphib</b></a> and <a href="../examples/jpegfromhib.html"><b>JPEGfromhib</b></a> to do a deeper inspection of the <tt>testimages.hib</tt> and <tt>testimages.hib.dat</tt> files.
      
	<h3>How downloader works</h3>
	
	The HIPI Distributed Downloader attempts to download a list of images located at URLs that are specified in an input ASCII text file with one URL per line. Because downloading a large set of images from a single computer can be slow and because typical cluster setups can support more bandwidth than an individual node, the downloader is designed to distribute this task to multiple nodes in a Hadoop cluster. In the following discussion, we assume that <b>downloader</b> is attempting to download <strong>k</strong> images using <strong>n</strong> download nodes and that the underlying Hadoop cluster is comprised of <strong>m</strong> compute nodes. Note that the number of download nodes requested by the user <strong>n</strong> is not necessarily equal to the number of compute nodes <strong>m</strong> though one would not expect any speed improvements after <strong>n</strong> exceeds <strong>m</strong> (i.e., full cluster utilization). <br /><br />

	To begin, let's study the class <tt>DownloaderInputFormat</tt> located in <tt>examples/hipi/examples/downloader/DownloaderInputFormat.java</tt>. This class extends the <a class="external_link" href="https://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.html"><tt>FileInputFormat</tt></a> Hadoop class and is responsible for parsing the input text file and initiating a map task for each image that is to be downloaded (each line in the input text file). In particular, the <tt>getSplits()</tt> method in this class causes <strong>n</strong> map tasks to each download (<strong>k</strong> / <strong>n</strong>) images into separate local <a href="../doc/api/hipi/imagebundle/HipiImageBundle.html"><tt>HipiImageBundle</tt></a> (HIBs). After these map tasks have completed, a single reduce task merges these HIBs into one HIB located at the output path specified by the user. This diagram illustrates the process:

	<img class="centered_image" src="downloader.png" width="500" alt="" />
	
	<h3><tt>DownloaderInputFormat</tt></h3> 

	Under normal operation, Hadoop attempts to execute each map task on whichever compute node in the cluster is &#39;closest&#39; to the input data for that task (keep in mind that the machines in a Hadoop cluster share the task of managing the distributed file system and performing distributed MapReduce computations). In the case of <b>downloader</b>, this is problematic because the input data is a small text file with a list of URLs, which will most likely be stored on a single physical machine in the cluster. Without taking steps to avoid the default behavior, Hadoop would download all of the images in serial using this one machine even if the cluster contained many nodes with separate fast connections to the Internet. In other words, Hadoop must be coerced into behaving differently in this case to fully utilize the cluster. <br /><br />

	<span class="important">Important Note:</span> This discussion is more related to how Hadoop works than HIPI, but you may still find it informative. If you're not familiar with how Hadoop works you may want to first read <a href="http://hadoop.apache.org/docs/stable/">some documentation</a>.<br /><br />

	In order to cause Hadoop to spawn <strong>n</strong> map tasks that each run on different nodes in the cluster (bear in mind this is only possible if <strong>m</strong> is greater than or equal to <strong>n</strong>), the <tt>getSplits()</tt> method in <tt>DownloaderInputFormat</tt> creates <strong>n</strong> temporary files on the HDFS. Due to Hadoop's effort to uniformly distribute data over the cluster, each new file has a high probability of being created on a different machine. Based on these observations, our process of spawning <strong>n</strong> map tasks on different machines is as follows:<br /><br />

	1. Create up to <strong>2n</strong> temporary files on the HDFS in the pursuit of identifying <strong>n</strong> unique compute nodes:

	    <pre id="Current">
   Configuration conf = job.getConfiguration();

   // Use a default value of 10 if 'downloader.nodes' is not explicitly set
   int nodes = conf.getInt("downloader.nodes", 10);

   ArrayList&lt;String&gt; hosts = <font id="New">new</font> ArrayList&lt;String&gt;(<font id="IntegerLiteral">0</font>);
   List&lt;InputSplit&gt; splits = <font id="New">new</font> ArrayList&lt;InputSplit&gt;();
  
   FileSystem fileSystem = FileSystem.get(conf);
   String tempOutputPath = conf.get(<font id="StringLiteral">"downloader.outpath"</font>) + <font id="StringLiteral">"_tmp"</font>;
   Path tempOutputDir = <font id="New">new</font> Path(tempOutputPath);
  
   // Ensure getSplits runs with a clean output directory
   <font id="If">if</font> (fileSystem.exists(tempOutputDir)) {
      fileSystem.delete(tempOutputDir, <font id="True">true</font>);
   }
   fileSystem.mkdirs(tempOutputDir);
  
   <font id="Int">int</font> i = <font id="IntegerLiteral">0</font>;
   <font id="While">while</font>( hosts.size() &lt; nodes &amp;&amp; i &lt; <font id="IntegerLiteral">2</font>*nodes) {
      String tempFileString = tempOutputPath + <font id="StringLiteral">"/"</font> + i;
      Path tempFile = <font id="New">new</font> Path(tempFileString);
      FSDataOutputStream os = fileSystem.create(tempFile);
      os.write(i);
      os.close();
	    </pre>

	    2. Determine which cluster node(s) each temporary file is stored on:

	    <pre id="Current">
      FileStatus match = fileSystem.getFileStatus(tempFile);
      <font id="Long">long</font> length = match.getLen();
      BlockLocation[] blocks = fileSystem.getFileBlockLocations(match, <font id="IntegerLiteral">0</font>, length);
	    </pre>

	    3. Ensure this node is unique in the current list of compute nodes (variable named <tt>hosts</tt> in the code), otherwise start over:
	    <pre id="Current">
      <font id="Boolean">boolean</font> save = <font id="True">true</font>;
      <font id="For">for</font> (<font id="Int">int</font> j = <font id="IntegerLiteral">0</font>; j &lt; hosts.size(); j++) {
         <font id="If">if</font> (blocks[<font id="IntegerLiteral">0</font>].getHosts()[<font id="IntegerLiteral">0</font>].compareTo(hosts.get(j)) == <font id="IntegerLiteral">0</font>) {
            save = <font id="False">false</font>;
            System.out.println(<font id="StringLiteral">"Repeated Host: "</font> + i);
            <font id="Break">break</font>;
         }
      }
	    </pre>

	    4. If the node was unique, save it in the list of unique nodes and repeat until the list contains <strong>n</strong> nodes or we have created <strong>2n</strong> temporary files, whichever comes first:
	    <pre id="Current">
      <font id="If">if</font> (save) {
         System.out.println(<font id="StringLiteral">"Host Found Successfully: "</font> + i);
         hosts.add(blocks[<font id="IntegerLiteral">0</font>].getHosts()[<font id="IntegerLiteral">0</font>]);
      }
      i++;
   } // while( hosts.size() < nodes && i < 2*nodes)
	    </pre>
	  </li>
	</ol>

	The resulting list of hosts can be used to generate a <a
	class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/lib/input/FileSplit.html">FileSplit</a>
	for each node. The FileSplit in this case is utilized in an
	unorthodox way to indicate to each Map node which portion of
	the URLs it will be responsible for. The constructor for the
	FileSplit normally takes four arguments: the path to the file,
	the starting byte offset for the split, the byte length of the
	split, and the list of hosts where the split resides. Instead
	of using this convention, the start offset and length
	parameters are substituted with the line offset and number of
	lines to be processed:
	<pre id="Current">
  <font id="Int">int</font> span = (<font id="Int">int</font>) Math.ceil(((<font id="Float">float</font>) (num_lines)) / ((<font id="Float">float</font>) hosts.size()));
  <font id="Int">int</font> last = num_lines - span * (hosts.size()-<font id="IntegerLiteral">1</font>);
  System.out.println(<span id="StringLiteral">"First n-1 nodes responsible for "</span> + span + <span id="StringLiteral">" images"</span>);
  System.out.println(<span id="StringLiteral">"Last node responsible for "</span> + last + <span id="StringLiteral">" images"</span>);
  
  FileSplit[] f = <font id="New">new</font> FileSplit[hosts.size()];
  <font id="For">for</font> (<font id="Int">int</font> j = <font id="IntegerLiteral">0</font>; j &lt; f.length; j++) {
      String[] host = <font id="New">new</font> String[<font id="IntegerLiteral">1</font>];
      host[<font id="IntegerLiteral">0</font>] = hosts.get(j);
      <font id="If">if</font> (j &lt; f.length - <font id="IntegerLiteral">1</font>) {
          splits.add( <font id="New">new</font> FileSplit(path , (j*span) , span, host));
      } <font id="Else">else</font> {
          splits.add( <font id="New">new</font> FileSplit(path , (j*span) , last, host));
      }
  }
	</pre>

	<h3>DownloaderRecordReader</h3>

	The <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a>
	is normally tasked with emitting a set of records for each <a
	class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a>
	it receives from a specified <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/InputFormat.html">InputFormat</a>. In
	the case of the Downloader, the DownloaderRecordReader is
	responsible for communicating the to the Map task which lines
	in the input list of URLs it is responsible for
	downloading. As mentioned above, the DownloaderInputFormat
	class uses the <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/lib/input/FileSplit.html">FileSplit</a>
	object to hold this information.<br /><br />

	For each FileSplit, the DownloaderRecordReader emits exactly
	one record whose key indicates the starting line number and
	whose value contains the list of URLs to be processed. The
	DownloaderRecordReader first determines the starting line
	number for the FileSplit and the number of lines that will be
	processed:
	<pre id="Current">
  ...
  <font id="Public">public</font> <font id="Void">void</font> initialize(InputSplit split, TaskAttemptContext context) <font id="Throws">throws</font> IOException, InterruptedException {
    FileSplit fileSplit = (FileSplit) split;
    Path path = fileSplit.getPath();
    FileSystem fileSystem = path.getFileSystem(context.getConfiguration());
  
    start_line = f.getStart();
    <font id="Long">long</font> num_lines = f.getLength();
	</pre>

	The corresponding URLs are determined by reading
	<strong>num_lines</strong> from the input file starting at
	line <strong>start_line</strong>:
	<pre id="Current">
  BufferedReader reader = <font id="New">new</font> BufferedReader(<font id="New">new</font> InputStreamReader(fileSystem.open(path)));
  <font id="Int">int</font> i = <font id="IntegerLiteral">0</font>;
  <font id="While">while</font>(i &lt; start_line &amp;&amp; reader.readLine() != <font id="Null">null</font>) {
    i++;
  }
  
  urls = <font id="StringLiteral">""</font>;
  String line;
  <font id="For">for</font>(i = <font id="IntegerLiteral">0</font>; i &lt; num_lines &amp;&amp; (line = reader.readLine()) != <font id="Null">null</font>; i++) {
    urls += line + <font id="CharacerLiteral">'\n'</font>;
  }
  reader.close();
	</pre>

	Map tasks receive their records by repeatedly calling <a
	class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html#nextKeyValue()">RecordReader::nextKeyValue()</a>,
	<a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html#getCurrentKey()">RecordReader::getCurrentKey()</a>,
	and <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html#getCurrentValue()">RecordReader::getCurrentValue()</a>. Thus,
	in this case, there is only one record whose key and value are
	just the starting line number and the list of URLs
	respectively:
	<pre id="Current">
  <font id="Public">public</font> IntWritable getCurrentKey() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="Return">return</font> <font id="New">new</font> IntWritable((<font id="Int">int</font>)start_line);
  }

  <font id="Public">public</font> Text getCurrentValue() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="Return">return</font> <font id="New">new</font> Text(urls);
  }

  <font id="Public">public</font> <font id="Boolean">boolean</font> nextKeyValue() <font id="Throws">throws</font> IOException, InterruptedException {
      <font id="If">if</font>(singletonEmit == <font id="False">false</font>) {
          singletonEmit = <font id="True">true</font>;
          <font id="Return">return</font> <font id="True">true</font>;
      } <font id="Else">else</font> {
          <font id="Return">return</font> <font id="False">false</font>;
      }
  }
	</pre>

	<h3>The Mapper</h3>

	The <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/RecordReader.html">RecordReader</a>
	emits records to the <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Mapper.html">Mapper</a>
	task where the keys are the starting line in the input list of
	URLs and the values are the URLs to be downloaded (separated
	again by newline characters). The Mapper's task is to download
	each of the URLs to a local <a
	href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundle</a>
	(HIB), which will then be sent to the <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer</a>
	so it can be combined with the other HIB's. A temporary HIB is
	created on each of the map nodes to hold the set of images it
	is responsible for downloading:
	<pre id="Current">
  <font id="Public">public</font> <font id="Void">void</font> map(IntWritable key, Text value, Context context) <font id="Throws">throws</font> IOException, InterruptedException
  {
    String temp_path = conf.get(<font id="StringLiteral">"downloader.outpath"</font>) + key.get() + <font id="StringLiteral">".hib.tmp"</font>;
    HipiImageBundle hib = <font id="New">new</font> HipiImageBundle(<font id="New">new</font> Path(temp_path), conf);
    hib.open(HipiImageBundle.FILE_MODE_WRITE, <font id="True">true</font>);
    ...
	</pre>

	Each of the Mapper's then proceeds to iterate through its set
	of URLs (contained in the value). To perform the actual
	download, the <a class="external_link"
	href="http://docs.oracle.com/javase/7/docs/api/java/net/URLConnection.html">URLConnection</a>
	is used to create a stream that can be written directly to the
	local HIB:
	<pre id="Current">
    String word = value.toString();
    BufferedReader reader = <font id="New">new</font> BufferedReader(<font id="New">new</font> StringReader(word));
    String uri;

    <font id="While">while</font>((uri = reader.readLine()) != <font id="Null">null</font>)            
    {
      ...
      String type = <font id="StringLiteral">""</font>;
      URLConnection conn;

      <font id="Try">try</font> {
        URL link = <font id="New">new</font> URL(uri);
        System.err.println(<font id="StringLiteral">"Downloading "</font> + link.toString());
	conn = link.openConnection();
	conn.connect();
	type = conn.getContentType();
      } <font id="Catch">catch</font> (Exception e) {
        System.err.println(<font id="StringLiteral">"Connection error to image: "</font> + uri);
        <font id="Continue">continue</font>;
      }
      ...
      <font id="If">if</font> (type != <font id="Null">null</font> &amp;&amp; type.compareTo(<font id="StringLiteral">"image/jpeg"</font>) == <font id="IntegerLiteral">0</font>) {
	hib.addImage(conn.getInputStream(), ImageType.JPEG_IMAGE);
      }
      ...
    }
	</pre>

	Once all of the URLs have been processed, the HIB must be
	closed and a record indicating its location on the filesystem
	is sent to the Reducer:
	<pre id="Current">
  context.write(<font id="New">new</font> BooleanWritable(<font id="True">true</font>), <font id="New">new</font> Text(hib.getPath().toString()));
  reader.close();
  hib.close();
  </pre>

	<h3>The Reducer</h3>

	The <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Reducer.html">Reducer's</a>
	job is relatively straighforward. It loads each of the <a
	href="../doc/api/hipi/imagebundle/HipiImageBundle.html">HipiImageBundles</a>
	(HIB) from the <a class="external_link"
	href="https://hadoop.apache.org/docs/r2.5.1/api/org/apache/hadoop/mapreduce/Mapper.html">Mappers</a>
	and merges them into one large HIB:
	<pre id="Current">
  <font id="Public">public</font> <font id="Void">void</font> reduce(BooleanWritable key, Iterable&lt;Text&gt; values, Context context) 
      <font id="Throws">throws</font> IOException, InterruptedException {
    <font id="If">if</font>(key.get()) {
      FileSystem fileSystem = FileSystem.get(conf);
      Path outputHibPath = <font id="New">new</font> Path(conf.get(<font id="StringLiteral">"downloader.outfile"</font>));
      HipiImageBundle hib = <font id="New">new</font> HipiImageBundle(outputHibPath, conf);
      hib.open(HipiImageBundle.FILE_MODE_WRITE, <font id="True">true</font>);
      <font id="For">for</font> (Text temp_string : values) {
        Path temp_path = <font id="New">new</font> Path(temp_string.toString());
	HipiImageBundle input_bundle = <font id="New">new</font> HipiImageBundle(temp_path, conf);
	hib.append(input_bundle);
	Path index_path = input_bundle.getPath();
	Path data_path = <font id="New">new</font> Path(index_path.toString() + <font id="StringLiteral">".dat"</font>);
        fileSystem.delete(index_path, false);
        fileSystem.delete(data_path, false);
	Text outputPath = <font id="New">new</font> Text(input_bundle.getPath().toString());
	context.write(<font id="New">new</font> BooleanWritable(<font id="True">true</font>), outputPath);
	context.progress();
      }
      hib.close();
    }
  }
	</pre>
      </div>
    </div>
    <!-- End Content -->
  </body>
</html>
